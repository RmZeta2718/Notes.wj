# 神经网络

## 一般理论

### 前向传播

输入： $a^{[l-1]}$ （$a^{[0]}$ 即神经网络输入 $A^{[0]}=X$ ）

输出： $a^{[l]}, cache(z^{[l]})$  

单个样本：
$$
\begin{align}
z^{[l]}&=w^{[l]}\cdot a^{[l-1]}+b^{[l]}\\
a^{[l]}&=g^{[l]}(z^{[l]})
\end{align}
$$

> 假设第 $l$ 层有 $n^{[l]}$ 个神经元，那么： $\mathrm{shape}(z^{[l]},a^{[l]},b^{[l]})=(n^{[l]},1)$ ， $\mathrm{shape}(a^{[l-1]})=(n^{[l-1]},1)$ ， $\mathrm{shape}(w^{[l]})=(n^{[l]},n^{[l-1]})$ 
>
> （线性方程视角）第 $l$ 层、第 $i$ 个神经元 $a^{[l]}_i$ （即第 $i$ 个输出）对应的权重是行向量 $w^{[l]T}_i$ ， $\mathrm{shape}(w^{[l]T}_i)=(1,n^{[l-1]})$ ， $w^{[l]}$ 由 $n$ 个行向量 $w^{[l]T}_i$ 纵向拼接而成。

向量化（一组样本）：
$$
\begin{align}
Z^{[l]}&=w^{[l]}\cdot A^{[l-1]}+b^{[l]}\\
A^{[l]}&=g^{[l]}(Z^{[l]})
\end{align}
$$

> 假设batch size是 $T$ ，那么： $\mathrm{shape}(Z^{[l]},A^{[l]})=(n^{[l]},T)$ ， $\mathrm{shape}(A^{[l-1]})=(n^{[l-1]},T)$ ，由 $T$ 个列向量样本横向拼接而成，例如：
> $$
> A^{[l]}=\begin{bmatrix}
>  | & | &  & | \\
> a^{[l](1)} & a^{[l](2)} & \cdots & a^{[l](T)} \\
>  | & | &  & |
> \end{bmatrix}
> $$
>  $+$ 运算理解为numpy中的broadcasting，即与 $(n^{[l]},T)$ 维度的矩阵相加时， $b^{[l]}$ 从 $(n^{[l]},1)$ 扩展为 $(n^{[l]},T)$ 

### 反向传播

记损失函数为 $J$ ， $da\triangleq \dfrac{\partial J}{\partial a}$ ，最大层数为 $L$

输入： $da^{[l]}$ （最后一层 $da^{[L]}$ 根据损失函数决定）， $cached(z^{[l]})$ 

输出： $da^{[l-1]}, dw^{[l]},db^{[l]}$  

单个样本：
$$
\begin{align}
dz^{[l]}&=da^{[l]}*g^{[l]}{'}(z^{[l]})\\
dw^{[l]}&=dz^{[l]}\cdot a^{[l-1]}\\
db^{[l]}&=dz^{[l]}\\
da^{[l-1]}&=W^{[l]T}\cdot dz^{[l]}
\end{align}
$$
向量化：
$$
\begin{align}
dZ^{[l]}&=dA^{[l]}*g^{[l]}{'}(Z^{[l]})\\
dW^{[l]}&=\dfrac1m dZ^{[l]}\cdot A^{[l-1]T}\\
db^{[l]}&=\dfrac1m \mathrm{np.sum}(dZ^{[l]},\mathrm{axis=1},\mathrm{keepdim=True})\\
dA^{[l-1]}&=W^{[l]T}\cdot dZ^{[l]}
\end{align}
$$

### 深度神经网络必要性

- 直觉来看：神经网络的浅层处理检测简单函数、提取简单特征，例如图形边缘、音调高低；而神经网络的深层将前面的特征组合在一起，例如人脸、单词（语音识别），于是在总体上学习到更加复杂的函数。
- 从卷积神经网络来看，浅层覆盖面积小，深层覆盖面积大
- 从表达能力角度，存在某些神经网络，如果要用更浅的神经网络计算，则需要指数级的额外神经元。
  - 以 $x_1\oplus x_2\oplus \cdots \oplus x_n$ 为例，如果用二叉树结构的神经网络，每个节点计算两个数的异或，该节点内部可能是两层神经网络或者若干级与非门电路，不妨记内部深度为 $m$ ，这样二叉树结构的神经网络的深度是 $O(m\log n)$ ，节点数是 $O(n)$ ；而如果强制用一层隐藏层的神经网络计算，为了完整表达所有情况，节点数是 $O(2^n)$ 

## 卷积神经网络 Convolutional（CNN）

### 卷积层

输入： $\mathrm{shape}(a^{[l-1]})=(n_H^{[l-1]},n_W^{[l-1]},n_c^{[l-1]})$ 

输出： $\mathrm{shape}(a^{[l]})=(n_H^{[l]},n_W^{[l]},n_c^{[l]})$ 

$f^{[l]}$ : filter size, $p^{[l]}$ : padding, $s^{[l]}$ : stride, $n_c^{[l]}$ : \#filters( \#channels)

 $n_{H/W}^{[l]}=\left \lfloor \dfrac{n_{H/W}^{[l-1]}+2p^{[l]}-f^{[l]}}{s^{[l]}}+1 \right \rfloor $

单个卷积核大小： $(f^{[l]},f^{[l]},n_c^{[l-1]})$ 

> convention: 上一层的channel和这一层的卷积核channel相等，即上述 $n_c^{[l-1]}$ 

 $\mathrm{shape}(a^{[l]})=(n_H^{[l]},n_W^{[l]},n_c^{[l]})$ 

 $\mathrm{shape}(w^{[l]})=(f^{[l]},f^{[l]},n_c^{[l-1]},n_c^{[l]})$ 

 $\mathrm{shape}(b^{[l]})=(1,1,1,n_c^{[l]})$ 

> 向量化略去

特点：

- 参数共享：边缘检测在图片的所有位置都可用
- 稀疏连接：两层之间的连接数量较少

### 池化层

Max pooling/Average pooling

没有可学习参数

## 残差网络 [[@2016DeepResidualLearningHe|ResNet]]

残差块 Residual block： $a^{[l+2]}=g(z^{[l+2]}\underline{+a^{[l]}})$ 

如果 $n^{[l+2]}\neq n^{[l]}$ ，在 $a^{[l]}$ 前乘上 $w_s$ ， $\mathrm{shape}(w_s)=(n^{[l+2]},n^{[l]})$ 

## U-Net

### 转置卷积 Transpose Convolution





# 深度学习

### 过/欠拟合

贝叶斯最优错误率（Bayes optimal error）：从x到y的理论最优映射

Human-level error：可以视作最优错误率的近似
$$
\text{Human-level error}\overset{\text{Bias}}{\longleftrightarrow}\text{Training error}\overset{\text{Variance}}{\longleftrightarrow}\text{Dev error}
$$

| 测试集误差 | 验证集误差 | 结论                      |
| ---------- | ---------- | ------------------------- |
| 1%         | 11%        | high variance，过拟合     |
| 15%        | 16%        | high bias，欠拟合         |
| 15%        | 30%        | high bias & high variance |
| 0.5%       | 1%         | low bias & low variance   |

应对：

欠拟合：更大的神经网络、训练更长时间、调整网络结构

过拟合：获取更多数据、考虑正则化、调整网络结构

在深度神经网络之前，降低欠拟合、降低过拟合两者互相矛盾。但是现在不再需要权衡，例如获取更多数据会单方面降低过拟合，而更大的神经网络，只要正确地正则化，就能单方面降低欠拟合，而不导致过拟合。于是，网络越来越大、数据集越来越大、神经网络性能越来越好。

### 正则化 Regularization

#### L2

$$
J(w^{[1]},b^{[1]},\cdots,w^{[L]},b^{[L]})=\dfrac1m \sum_{i=1}^mL(\hat y^{(i)},y^{(i)})+\underline{\dfrac\lambda{2m}\sum_{l=1}^L\left\| w^{[l]} \right \|^2_F}{}
$$

其中， $m$ 是样本数量， $\lambda$ 是正则化系数，矩阵的F-范数（Frobenius norm）是所有元素的平方和，即
$$
\left\| w^{[l]} \right \|^2_F=\sum_{i=1}^{n^{[l-1]}}\sum_{j=1}^{n^{[l]}}\left(w_{ij}^{[l]}\right)^2
$$
加入正则化的反向传播 $dw^{[l]}_R$ 在原有基础上加上额外一项：
$$
dw^{[l]}_R=dw^{[l]}+\dfrac\lambda m w^{[l]}
$$
于是 $w^{[l]}$ 的梯度下降过程变为：
$$
\begin{align}
w^{[l]}&:=w^{[l]}-\alpha\cdot dw^{[l]}_R\\
&=w^{[l]}-\alpha\left[dw^{[l]}+\dfrac\lambda m w^{[l]}\right]\\
&=(1-\dfrac {\alpha\lambda}m)w^{[l]}-\alpha\cdot dw^{[l]}
\end{align}
$$
其中， $\alpha$ 是学习率

#### dropout（随机失活）

> training only, not in testing

Inverted dropout

```python
keep_prob = 0.8
d = np.random.rand(a.shape[0], a.shape[1]) < keep_prob
a = np.multiply(a, d)  # a *= d
a /= keep_prob  # inverted
```

 $z^{[l]}=w^{[l]}a^{[l-1]}+b^{[l]}$ ，补偿 $a^{[l-1]}$ ，使期望不变

**Intuition**

每次dropout不同的上游，因此不能依赖于任何一个特定的上游，于是权重值倾向于分散在每一个上游节点上，导致权重变小（类似L2正则）

#### 其他正则化方法

数据增强也算是一种正则化

early stopping：在泛化误差变大之前终止训练

- 对比L2-norm：需要grid search $\lambda$ 

## 训练加速技巧

### 归一化 Normalize input

训练集和测试集要用同一组参数归一化

### Batch Norm

每一层都对 $a^{[l]}$ 或 $z^{[l]}$ 正则化，以 $z^{[l]}$ 为例，训练时：
$$
\begin{align}
\mu&=\dfrac1T\sum_iz^{[l](i)}\\
\sigma^2&=\dfrac1T\sum_i(z^{[l](i)}-\mu)^2\\
z_{norm}^{[l](i)}&=\dfrac{z^{[l](i)}-\mu}{\sqrt{\sigma^2+\varepsilon}}\\
\tilde z^{[l](i)}&=\gamma\cdot z_{norm}^{[l](i)}+\beta
\end{align}
$$
 $z_{norm}^{[l](i)}$ 即为batch norm后的值。也可以取 $\tilde z^{[l](i)}$ ，其中 $\gamma,\beta$ 是可学习的参数，当 $\gamma=\sqrt{\sigma^2+\varepsilon},\beta=\mu$ 时， $\tilde z^{[l](i)}=z^{[l](i)}$ 。

> 注意到 $z^{[l]}=w^{[l]}\cdot a^{[l-1]}+b^{[l]}$ ，由于batch norm会减去 $\mu$ ，故如果用了batch norm，b参数就不需要了

测试时：一次只有一个样本

通过指数加权移动平均估计 $\mu,\sigma^2$ ，对训练时的每个batch取移动平均。

### Layer Norm

每一个样本都对 $a^{[l](t)}$ 或 $z^{[l](t)}$ 正则化

$$
\begin{align}
\mu&=\sum_iz^{[l](t)}_i\\
\sigma^2&=\sum_i(z^{[l](t)}_i-\mu)^2\\
z_{norm}^{[l](t)}&=\dfrac{z^{[l](t)}-\mu}{\sqrt{\sigma^2+\varepsilon}}
\end{align}
$$

### BN vs LN

按照[[#一般理论]]中的记号，一个batch的（第 $l$ 层的）神经元用表达为 $Z$ 和 $A$，$\mathrm{shape}(Z^{[l]},A^{[l]})=(n^{[l]},T)$，每一列是一个样本的所有神经元，或称这个样本（在第 $l$ 层）的特征（feature）向量，共 $T$ 列这样的特征构成一个batch。

Batch Norm（BN）是按照行归一化，对每个特征归一化

Layer Norm（LN）是按照列归一化，对每个样本归一化

### 梯度消失/梯度爆炸 vanishing/exploding gradients

**Intuition**

简单起见，激活函数取线性 $g(z)=z$ 

那么对于一个 $L$ 层神经网络
$$
\hat y=w^{[L]}\cdots w^{[2]}w^{[1]}x
$$
考虑第 $l$ 层的神经元，简单起见，令 $b^{[l]}=\boldsymbol{0},n^{[l]}=1,n^{l-1}\gg 1$ 
$$
\begin{align}
z^{[l]}&=w^{[l]}a^{[l-1]}+b^{[l]}\\
&=\sum_{i=1}^{n^{[l-1]}}w_{1i}^{[l]}a_i^{[l-1]}
\end{align}
$$
为了缓解梯度消失/爆炸， $n^{[l-1]}$ 越大， $w^{l}$ 就要越小。

**solution**

> 缓解、但不完全解决该问题

 $\mathrm{Var}(w_i)=\dfrac 1{n^{[l-1]}}$  

```python
w = np.random.randn(shape) * np.sqrt(1 / n)  # standard normal distribution
```

对于ReLU： $\mathrm{Var}(w_i)=\dfrac 2{n^{[l-1]}}$ 

tanh： $\mathrm{Var}(w_i)=\dfrac 1{n^{[l-1]}}$ （Xavier initialization）

也有： $\mathrm{Var}(w_i)=\dfrac 2{n^{[l-1]}+n^{[l]}}$ 

也可以在乘 $\mathrm{Var}$ 的时候给这个方差一个超参数

```python
w = np.random.randn(shape) * alpha * np.sqrt(1 / n)  # standard normal distribution
```

https://machinelearningmastery.com/weight-initialization-for-deep-learning-neural-networks/

### batch / mini-batch

- batch：每次梯度下降跑完整个数据集
- mini-batch：每次梯度下降一部分数据（如 $\mathrm{batchsize}=T$ ）
  -  $\mathrm{batchsize}=m$ ：batch gradient descend
  -  $\mathrm{batchsize}=1$ ：stochastic gradient descend（随机梯度下降），失去了向量化的加速

### 指数加权移动平均 Exponentially weighted moving average

对数组 $a_i$ 指数加权平均：
$$
\begin{align}
b_0&=0\\
b_i&=\lambda b_{i-1}+(1-\lambda)a_i\\
&=(1-\lambda)a_i+(1-\lambda)\lambda a_{i-1}+\cdots+(1-\lambda)\lambda^na_{i-n}+\cdots
\end{align}
$$
 $\displaystyle\lim_{\lambda\rightarrow1}\lambda^{\frac 1{1-\lambda}}=\dfrac1e$ ，因此可以估计 $b_i$ 大约是 $\dfrac1{1-\lambda}$ 个 $a_i$ 的平均值



**Bias correction**

前若干项偏小，如果确实关心前几项的具体值，可以加入偏差修正：

取 $c_i$ 作为修正后的结果： $c_i=\dfrac{b_i}{1-\lambda^i}$

### 动量梯度下降 Gradient descent with momentum

用指数加权移动平均更新 $dw,db$ ，一般取 $\lambda=0.9$ 

作用：当梯度比较震荡时（如椭圆等高线），可以平滑训练过程

注意到： $b_i=(1-\lambda)\sum_j\lambda^ja_{i-j}$ 。如果令 $b_i=\lambda b_{i-1}+a_i$ ，则 $b_i=\sum_j\lambda^ja_{i-j}$ ，相当于整体放缩 $\dfrac1{1-\lambda}$ 倍。对于梯度下降来说，等价于学习率放缩。（不建议）

> Q：椭圆等高线的直观例子是否可以推广到高维？

### Root Mean Square Prop（RMSProp）

对梯度平方（Square）、取平均（Mean）、更新时除以平均值的平方根（Root）
$$
\begin{align}
S&=\lambda S+(1-\lambda)dw^2\\
w&:=w-\alpha\dfrac{dw}{\sqrt{S}+\varepsilon}
\end{align}
$$
加 $\varepsilon$ 是为了数值稳定性（numerical stability），防止除以0。平方和根号都是标量运算。



### Adam（Adaptive Moment Estimation，自适应矩估计）

$$
\begin{align}
&\text{Initialization:}\\
&\qquad V_{dw}=S_{dw}=0\\
&\text{On iteration t:}\\
&\qquad V_{dw}=\beta_1V_{dw}+(1-\beta_1)dw\\
&\qquad S_{dw}=\beta_2S_{dw}+(1-\beta_2)dw^2\\
&\qquad V_{dw}^{*}=V_{dw}/(1-\beta_1^t)\qquad (Bias correction)\\
&\qquad S_{dw}^{*}=S_{dw}/(1-\beta_2^t)\\
&\qquad w:=w-\alpha \dfrac{V_{dw}^*}{\sqrt{S_{dw}^*}+\varepsilon}
\end{align}
$$



|        超参数 | 常用值          |
| ------------: | :-------------- |
|      $\alpha$ | 需要grid search |
|     $\beta_1$ | $0.9$           |
|     $\beta_2$ | $0.999$         |
| $\varepsilon$ | $10^{-8}$       |

**Intuition**

假设 $w$ 是随机变量，那么Adam的本质就是估计一个序列的 $\dfrac{\text{mean}(w)}{\text{std}(w)e}$ ，估计方式为对分子分母分别指数加权移动平均。

### 学习率衰减 Learning rate decay

d: decay rate
$$
\alpha=\dfrac1{1+\text{decay rate}\times \text{epoch}}\alpha_0\\
\alpha=0.95^{\text{epoch}}\cdot\alpha_0\\
\alpha=\dfrac k{\sqrt{\text{epoch}}}\cdot\alpha_0
$$


## 调试技巧

### 梯度检验 Gradient Checking

> debugging only, not in traininefefefef g

数值计算：双边比单边的近似程度更好
$$
\begin{align}
f'(x)&=\dfrac{f(x+\varepsilon)-f(x)}{\varepsilon}+O(\varepsilon)\\
&=\dfrac{f(x+\varepsilon)-f(x-\varepsilon)}{2\varepsilon}+O(\varepsilon^2)
\end{align}
$$
记 $\theta\triangleq (w^{[1]T},b^{[1]T},\cdots,w^{[L]T},b^{[L]T})^T,d\theta\triangleq\dfrac {\part J(\theta)}{\part \theta}$ 

> 损失函数 $J(\theta)$ 应当包含正则化项（如有）

 $d\theta_i=\dfrac {\part J(\theta)}{\part \theta_i}\approx\dfrac{J(\theta_1,\cdots,\theta_i+\varepsilon,\cdots)-J(\theta_1,\cdots,\theta_i-\varepsilon,\cdots)}{2\varepsilon}\triangleq  d\hat\theta_i$ 

对于某个 $\varepsilon$ 例如 $10^{-7}$ 校验：  $\dfrac{\|d\hat\theta-d\theta\|_2}{\|d\hat\theta\|_2+\|d\theta\|_2}$ 与 $\varepsilon$ 的量级是否接近

> 梯度检验不适用于dropout（需要暂时关闭dropout）
>
> 有时 $\|\theta\|$ 较大时才会检测出问题（需要训练一段时间）

## 局部最优点

通常局部最优点（local optima）不存在，因为梯度为0的点大概率是鞍点（saddle point）：所有维度上的二阶导数都同号才会是局部最优点，当维度很高时，这种情况发生的概率很低。

> 低维度中的一些直觉不适用于高维度。

大面积的小梯度区域，即平稳段（plateau），会导致学习速度慢。通常用Adam等优化方法可以更快地走出平稳段

## 炼丹技巧

### Grid search: Don't use grid

在超参数空间中随机取点，而不是网格化取点。因为不知道哪个参数更重要。

逐渐缩小搜索范围。

### 机器学习策略

#### 正交化

正交化地控制变量

- 训练集性能
  - 更大的网络
  - 更好的Optimizer
- 开发集性能
  - 正则化
  - 更大的数据集
- 测试集性能（过拟合开发集）
  - 更大的数据集
- 现实世界性能
  - 更换开发集
  - 更换损失函数

## 训练集与验证集来自不同分布

引入Traning-dev set，与训练集来自同一分布

比较Traning-dev set与Dev set上的表现，就能知道是模型的问题还是数据集分布不同的问题。

## 训练范式

### 迁移学习 Transfer Learning

预训练+微调。

预训练任务数据量大，微调任务数据量小。

低层网络共享知识时比较有效。

### 多任务学习 Multi-task Learning

一次输出多个标签

每个任务的数据量相当

低层网络共享知识时比较有效。

### 端到端学习 end-to-end Leaning

一些机器学习系统需要多个阶段的处理，端到端学习用一个大神经网络替换所有阶段。例如：

audio->features->...->transcripts

audio------->transcripts

- 优点：
  - 让数据说话
    - 对比多阶段，中间阶段是基于人类认知设立的，不一定是最佳方案，例如拆分为音节等
- 缺点：
  - 需要大量数据（子阶段通常更独立，有更多数据）
  - 无法注入人类认知

# 线性模型

### 逻辑回归Logistic Regression

#### 模型

给出 $x\in\mathbb R^n$ ，求标签为1的概率 $\hat y=P(y=1|x)$ ， $\hat y\in[0,1]$ ，标签 $y\in\{0,1\}$ 

参数 $w\in\mathbb R^n,b\in\mathbb R$ 

输出 $\hat y=\sigma(w^Tx+b)\triangleq\sigma(z)$ ，其中 $\sigma(z)=\dfrac 1{1+e^{-z}}$ 

> 不要使用扩展的记法 $\hat w=(w^T,b)^T$ ，因为代码实现更接近 $w,b$ 分开的记法

$P(y|x)=\hat y^y(1-\hat y)^{(1-y)}$ 

#### 策略

损失函数：

- bad choice：最小平方误差MSE $\mathcal L(\hat y,y)=\frac 12(\hat y-y)^2$ ，导致非凸，多个局部最优点
- good choice：交叉熵 $\mathcal L(\hat y,y)=-\left(y\log\hat y+(1-y)\log(1-\hat y)\right)$ ，凸，等价于 $\log P(y|x)$ 的最大似然估计MLE

总体cost： $J(w,b)=\frac 1m\sum_{i=1}^m\mathcal L(\hat y^{i},y^{i})$ 



#### 算法

梯度下降：

 $\left\{\begin{matrix}
w:=w-\alpha \dfrac{\mathrm \part J(w,b)}{\mathrm \part w} \\
b:=b-\alpha \dfrac{\mathrm \part J(w,b)}{\mathrm \part b}
\end{matrix}\right.$ 

### 感知机Perception





## 激活函数

### 线性激活函数

恒等激活函数 $a=z$ ：不可用，导致神经网络变成线性模型。

### sigmoid

 $\sigma(z)=\dfrac 1{1+e^{-z}}$ 

 $\sigma'(z)=\sigma(z)(1-\sigma(z))$ 

一般只用于0/1二分类的输出层

Sigmoid是一维的Softmax

### tanh

 $\tanh(z)=\dfrac {e^z-e^{-z}}{e^z+e^{-z}}$ 

 $\tanh'(z)=1-\tanh^2(z)$

向下平移后的sigmoid，经过原点，均值为0，因此一般总是优于sigmoid，方便下一层的学习。

上述两种的问题： $z$ 很大或很小时，梯度几乎为0，使梯度下降收敛太慢。

### ReLU（Rectified Linear Unit）

 $a=\max(0,z)$ ，解决梯度问题，一般总是优于上面两种。新的问题：

-  $z=0$ 时不可导。实践中概率极低（浮点数精确=0），可以手动赋值梯度为0或1。
-  $z<0$ 时梯度为0。实践中没什么问题。有足够多的大于零的数据。折中方案：

Leaky ReLU： $a=\max(0.01z, z)$ ：解决梯度为0的问题。

### Softmax

> https://www.bilibili.com/video/BV1FT4y1E74V?p=79

$\mathbb R^{n\times1}\longmapsto\mathbb R^{n\times1}$ ，在一整层上的激活函数，而非单个神经元的激活函数

 $a^{[l]}=\dfrac{e^{z^{[l]}}}{\sum_ie^{z^{[l]}_i}}$

一些解释：

- 取指数（exp）：让所有值变成正数，且扩大较大值的影响
- 除以一个和：归一化，让结果落在 $(0,1)$ 内



# 一般方法

## 没有免费的午餐定理（No Free Lunch Theorem, NFL）



符号：

- 输入空间： $\mathcal X$ ，输出空间： $\mathcal Y$ 
- 真实目标模型是 $f(\cdot)$ ，学习到的模型（或称“假设”）是 $h(\cdot)$ ， $f,h: \mathcal X \longmapsto \mathcal Y$ ，来自假设空间，即 $f,h\in\mathcal H=\mathcal X\times\mathcal Y$ 
-  $\mathcal L$ 表示训练时采用的算法
-  $X\sub \mathcal X$ 表示训练数据集
-  $\mathbb I(\cdot): \mathrm{Boolean} \longmapsto \{0,1\}$ 是指示函数

假设：

1. 简单起见，假设输入空间 $\mathcal X$ 和假设空间 $\mathcal H$ 是离散的
2. 仅考虑二分类问题，即假设输出空间 $\mathcal Y=\{0,1\}$ 
3.  $f(\cdot)$ 在所有可能中均匀分布，即

目标：假设真实目标（ground-truth） $f(\cdot)$ 在所有可能性中均匀分布，证明通过训练集 $X$ 和算法 $\mathcal L$ 学习到的模型 $h(\cdot)$ 在所有可能的 $f(\cdot)$ 下的训练集外平均错误率 $\overline E$ 与算法 $\mathcal L$ 无关。

证明：

 $\mathbb{I}(h(\boldsymbol x)\ne f(\boldsymbol x))$ 表示在某个模型 $h(\cdot)$ 和某个样本 $\boldsymbol x\in\mathcal X$ 下，预测值 $h(\boldsymbol x)$ 是否错误。这一项在所有情况下的平均值就是平均错误率 $\overline E$ 。

对于某一个真实模型 $f$ ，考虑所有训练集外样本 $\boldsymbol x\in\mathcal X-X$ 和所有模型 $h$ 的平均错误率 $E$ ，对这两项分别加权求，权重为出现的概率：

-  $\boldsymbol x$ 在 $\mathcal X-X$ 中出现的概率 $P(\boldsymbol x)$ 
- 根据训练数据集 $X$ 和学习算法 $\mathcal L$ ，模型 $h(\cdot)$ 在所有模型中出现的条件概率 $P(h|X,\mathcal L)$ 

> 根据假设1，总和表达为离散和。否则（若为连续型），则表达为积分

$$
E_{ote}=\sum_{\boldsymbol x\in\mathcal X-X}P(\boldsymbol x)\sum_hP(h|X,\mathcal L)\cdot\mathbb{I}(h(\boldsymbol x)\ne f(\boldsymbol x))
$$



证明过程参考西瓜书P8

## 机器学习特点

- 从数据中学习统计规律，并使用模型进行预测
- 不能保证学习到有用模型
- 数据质量很重要（垃圾数据进，垃圾结果出）
- 学到的是相关关系，而不是因果关系
- 没有免费的午餐：没有一种方法包打天下

### 机器学习局限性

- 依赖于数据
- 需要大量计算资源
- 数据不能揭示一切
- 统计规律不代表因果关系
