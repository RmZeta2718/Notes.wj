# 采坑记录

记录格式：

### 标题可以是问题类型或出错的模块
> 22/03/26（记录时间）

```
报错信息
```

排查：可能需要的排查流程

原因：分析问题原因

解决方案：

- 写明解决的步骤 1
- 步骤 2
- 。。。

参考资料：

### 显存不足
> 22/03/26

```
RuntimeError: CUDA out of memory.Tried to allocate 20.00 MiB
```

排查： `nvidia-smi` 查看显卡状况

解决方案：

- 将 batchsize 改小一些
- 根据显卡状况，更换 GPU id

### nn.DataParallel BUG
> 22/03/26

```
StopIteration: Caught StopIteration in replica 0 on device 0.
```

原因：pytorch 单机多卡用 nn.DataParallel 的时候无法 forward，会报错。pytorch1.5 的 bug

解决方案：

- 不用 nn.DataParallel
- 或者降级到 pytorch==1.4

参考：https://blog.csdn.net/sunflower_sara/article/details/109674853

# 一些灵异事件

> 影响不大但是很奇怪的事情

### torch.Tensor.sum() undocumented keyword `keepdims`
> 23/01/31

 [pytorch 文档](https://pytorch.org/docs/stable/generated/torch.Tensor.sum.html#torch.Tensor.sum) 中的 keyword argument 是 `keepdim` 但是 `keepdims` 竟然也可以（可能是历史原因？）

不过 [numpy 文档](https://numpy.org/doc/stable/reference/generated/numpy.sum.html) 里是 `keepdims` ，也有可能是把未知 `kwargs` 向 numpy 透传了
