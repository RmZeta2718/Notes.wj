---
~
---

ç›¸å…³è®ºæ–‡æ”¶é›†ï¼š[Xnhyacinth/Awesome-LLM-Long-Context-Modeling: ğŸ“° Must-read papers and blogs on LLM based Long Context Modeling ğŸ”¥](https://github.com/Xnhyacinth/Awesome-LLM-Long-Context-Modeling#5-length-extrapolation)

[[@2023EfficientStreamingLanguageXiao#ç›¸å…³ç ”ç©¶|Efficient Streaming Language Models with Attention Sinks, 2023]] æå‡ºäº† Long Context çš„ä¸‰ä¸ªæ–¹å‘ï¼šé•¿åº¦å¤–æ¨ï¼ˆLength Extrapolationï¼‰ã€æ‰©å±•ï¼ˆé¢„è®­ç»ƒï¼‰ä¸Šä¸‹æ–‡çª—å£ï¼ˆContext Window Extensionï¼‰ã€æœ‰æ•ˆåˆ©ç”¨é•¿æ–‡æœ¬ï¼ˆImproving LLMsâ€™ Utilization of Long Textï¼‰

**é•¿åº¦å¤–æ¨ï¼ˆLength Extrapolationï¼‰ï¼š**

- [[@2022TrainShortTestPress|ALiBi]]ï¼ˆ[[@2022TrainShortTestPress|Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation, 2022]]ï¼‰
    - ä¸€ç§æ–°çš„ attention è®¡ç®—æ–¹å¼ã€‚
    - é‡‡ç”¨çš„è®¾å®šæ˜¯ä¸å¾®è°ƒçš„å¤–æ¨ã€‚

why Extrapolation doesn't work:

- Transformers overfit to positionsï¼ˆ [ALiBi enables transformer language models to handle longer inputs - YouTube](https://youtu.be/Pp61ShI9VGc?t=1266) ï¼‰ï¼Œç”¨åˆ°äº†æ²¡è®­ç»ƒè¿‡çš„ä½ç½®ç¼–ç 
- é¢„æµ‹çš„æ—¶å€™æ³¨æ„åŠ›æœºåˆ¶æ‰€å¤„ç†çš„ token æ•°é‡è¿œè¶…è®­ç»ƒæ—¶çš„æ•°é‡ï¼ˆ [Transformer å‡çº§ä¹‹è·¯ï¼š7ã€é•¿åº¦å¤–æ¨æ€§ä¸å±€éƒ¨æ³¨æ„åŠ› - ç§‘å­¦ç©ºé—´|Scientific Spaces](https://spaces.ac.cn/archives/9431#%E8%B6%85%E5%BC%BA%E5%9F%BA%E7%BA%BF) ï¼‰ã€‚åº”å¯¹ï¼šä¹˜ $\log n$ ï¼ˆ [ä»ç†µä¸å˜æ€§çœ‹ Attention çš„ Scale æ“ä½œ - ç§‘å­¦ç©ºé—´|Scientific Spaces](https://spaces.ac.cn/archives/8823) ï¼‰

**æ‰©å±•ï¼ˆé¢„è®­ç»ƒï¼‰ä¸Šä¸‹æ–‡çª—å£ï¼ˆContext Window Extensionï¼‰ï¼š**

- å·¥ç¨‹ä¼˜åŒ–ï¼š
    - [[@2022FlashAttentionFastMemoryEfficientDao|FlashAttention]]
    - [[@2023RingAttentionBlockwiseLiua|Ring Attention with Blockwise Transformers for Near-Infinite Context, 2023]]
- RNN ç›¸å…³çš„æ–¹æ³•ï¼š
    - [[@2019TransformerXLAttentiveLanguageDai|Transformer-XL]]
    - [[@2023ScalingTransformer1MBulatov|Scaling Transformer to 1M tokens and beyond with RMT, 2023]]
- é«˜æ•ˆï¼ˆè¿‘ä¼¼ / ç¨€ç–ï¼‰ Attentionï¼š
    - [[@2020LongformerLongDocumentTransformerBeltagy|Longformer]]ï¼šsparse attention
    - BigBird
    - [[@2022RethinkingAttentionPerformersChoromanski|Performer]]
    - [[@2023LongNetScalingTransformersDing|LongNet: Scaling Transformers to 1,000,000,000 Tokens, 2023]]
        - dilated attention
    - [[@2022SimpleLocalAttentionsXiong|Simple Local Attentions Remain Competitive for Long-Context Tasks, 2022]]
    - (Sliding/Blockwise) Window Attention ä»å¤´é¢„è®­ç»ƒ
- [[@2023ExtendingContextWindowChen|RoPE-PI]]ï¼ˆ[[@2023ExtendingContextWindowChen|Extending Context Window of Large Language Models via Positional Interpolation, 2023]]ï¼‰ï¼šé€šè¿‡ä½ç½® id çš„å†…æ’+å¾®è°ƒå®ç°æ›´é•¿çš„ä¸Šä¸‹æ–‡é•¿åº¦
    - [[@2024LongLoRAEfficientFinetuningChen|LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models, 2024]]
        - é™ä½ PI è®­ç»ƒæˆæœ¬ï¼šPI + åˆ†å—æ³¨æ„åŠ›(S2Attn) + PEFT(LoRA+Norm+Embed)
    - [[@2024LLMMaybeLongLMJin|SelfExtend]]ï¼ˆ[[@2024LLMMaybeLongLMJin|LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning, 2024]]ï¼‰
        - è¿‘è·ç¦»ç²¾ç¡® ID+è¿œè·ç¦»å†…æ’ ID
        - [Transformer å‡çº§ä¹‹è·¯ï¼š12ã€æ— é™å¤–æ¨çš„ ReRoPEï¼Ÿ - ç§‘å­¦ç©ºé—´|Scientific Spaces](https://spaces.ac.cn/archives/9708)
- [[@2023EffectiveLongContextScalingXiong|ABF]] (adjusted base frequency)ï¼ˆ[[@2023EffectiveLongContextScalingXiong|Effective Long-Context Scaling of Foundation Models, 2023]]ï¼‰
    - [[@2024ExtendingLLMsContextZhanga|Extending LLMs' Context Window with 100 Samples, 2024]]
    - è§£é‡Šï¼š
        - [[@2024ScalingLawsRoPEbasedLiu|Scaling Laws of RoPE-based Extrapolation, 2024]]
        - [[@2024BaseRoPEBoundsMen|Base of RoPE Bounds Context Length, 2024]]
- [[@2023YaRNEfficientContextPeng|YaRN]]
    - [Transformer å‡çº§ä¹‹è·¯ï¼š10ã€RoPE æ˜¯ä¸€ç§ Î² è¿›åˆ¶ç¼–ç  - ç§‘å­¦ç©ºé—´|Scientific Spaces](https://spaces.ac.cn/archives/9675)
- [[@2024LongRoPEExtendingLLMDing|LongRoPE]]ï¼ˆ[[@2024LongRoPEExtendingLLMDing|LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens, 2024]]ï¼‰
    - PI/ABF è°ƒå‚

**æœ‰æ•ˆåˆ©ç”¨é•¿æ–‡æœ¬ï¼ˆImproving LLMsâ€™ Utilization of Long Textï¼‰ï¼š**

- æš‚æ— 

RAG:

- [[@2023UnlimiformerLongRangeTransformersBertsch|Unlimiformer: Long-Range Transformers with Unlimited Length Input, 2023]]
- [[@2023FocusedTransformerContrastiveTworkowski|LongLLaMA]]ï¼ˆ[[@2023FocusedTransformerContrastiveTworkowski|Focused Transformer: Contrastive Training for Context Scaling, 2023]]ï¼‰
    - å¯¹æ¯”å­¦ä¹ +RAG

New model:

- SSM (structured state space models): [[@2023MambaLinearTimeSequenceGu|Mamba]]

TODO:

- [[@2023LearningCompressPromptsMu|Learning to Compress Prompts with Gist Tokens, 2023]]
    - soft
- [[@2023DissectingTransformerLengthChi|Sandwich]]ï¼ˆ[[@2023DissectingTransformerLengthChi|Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis, 2023]]ï¼‰
- [[@2022ExploringLengthGeneralizationAnil|Exploring Length Generalization in Large Language Models, 2022]]
- [[@2023LandmarkAttentionRandomAccessMohtashami|Landmark Attention: Random-Access Infinite Context Length for Transformers, 2023]]
- [[@2023PoSEEfficientContextZhu|PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training, 2023]]
    - æ‰©å±• PEï¼ˆ2048 -> 4096ï¼‰ï¼Œå’Œ [[@2023ExtendingContextWindowChen|interpolation]] ä¸€æ ·éœ€è¦ fine tune æ•´ä¸ªæ¨¡å‹ã€‚ä½† fine tune çš„æ—¶å€™ä¸éœ€è¦æŠŠçª—å£æ‰©å±•ï¼ˆä¿æŒ 2048 ä¸å˜ï¼‰ï¼ŒæŠŠä¸­é—´å»æ‰ä¸€äº› positionï¼ˆskipped positionï¼‰ã€‚
- RWKV: Capturing Long-range Dependencies in RWKV
- [[@2023LMInfiniteSimpleOntheFlyHan|LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models]]
    - focuses on "on-thefly" length generalization for non-fine-tuned models.
    - æå‡º Î» å½¢çŠ¶çš„ attn mask
    - æœ‰é•¿åº¦å’Œç†µæ–¹é¢çš„ç†è®ºåˆ†æ
- [[@2023M4LEMultiAbilityMultiRangeKwan|M4LE: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models, 2023]]
- [[@2024LeaveNoContextMunkhdalai|Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention, 2024]]
    - å‹ç¼©
- [[@2023AugmentingLanguageModelsWang|Augmenting Language Models with Long-Term Memory, 2023]]
    - side tuning ï¼ˆ[[@2022LSTLadderSideTuningSung|LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning, 2022]]ï¼‰
- [[@2024MegalodonEfficientLLMMa|Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length, 2024]]
    - å½’ä¸€åŒ–æ³¨æ„åŠ›
- [[@2024MitigatePositionBiasYu|Mitigate Position Bias in Large Language Models via Scaling a Single Dimension, 2024]]
    - è§£é‡Šæ€§å·¥ä½œï¼Œéšå±‚æŸä¸€ç»´çš„æ“ä½œï¼Œcasual masking å¸¦æ¥ bias
- Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon
- Extending Context Window of Large Language Models from a Distributional Perspective
- Evaluation of similarity-based explanations

[iclr24]In-Context Pretraining: Language Modeling Beyond Document Boundaries
[iclr24]Retrieval meets Long Context Large Language Models
[iclr24]What Algorithms can Transformers Learn? A Study in Length Generalization
[iclr24]Functional Interpolation for Relative Positions improves Long Context Transformers
[iclr24]On the Limitations of Temperature Scaling for Distributions with Overlaps, è¿™ç¯‡æ˜¯è¯´ TS æ˜¯ä¸€ç§è§£å†³ overconfidence çš„ calibration æ–¹æ³•ï¼Œä½†ä¹Ÿæœ‰ç¼ºç‚¹ã€‚Head Scale åœ¨ length generalization ä¸Šæœ‰æ•ˆæ˜¯å¦å¯ä»¥ä»è¿™ä¸ªè§’åº¦å†çœ‹çœ‹ï¼Ÿ
[iclr24]Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs, å¦‚æœåç»­è¦ç»§ç»­æ“ä½œ attention head,æ˜¯å¦å¯ä»¥åšç±»ä¼¼çš„æ“ä½œ

æ•°æ®ï¼š

- [[@2024DataEngineeringScalingFu|Data Engineering for Scaling Language Models to 128K Context, 2024]]
- [[@2024LongContextNotChen|Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models, 2024]]
- Selecting Influential Samples for Long Context Alignment via Homologous Models' Guidance and Contextual Awareness Measurement

- æ³¨æ„åŠ›æ¨¡å¼åˆ†æï¼š
    - æ¨ç†æ—¶ kv-cache å‹ç¼©ï¼š
    - [[@2024ModelTellsYouGe|Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs, 2024]]
        - åˆ†æ attention head çš„æ¨¡å¼ï¼Œé’ˆå¯¹ä¸€äº›å›ºå®šæ¨¡å¼å¯ä»¥è¿›è¡Œ KV cache çš„å‹ç¼©ï¼Œä»è€ŒåŠ é€Ÿæ¨ç†
        - ä¸¤ç¯‡ç±»ä¼¼æ–¹æ³•ï¼š
        - [[@2023_2HeavyHitterOracleZhang|Heavy Hitter]]ï¼ˆ[[@2023_2HeavyHitterOracleZhang|H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models, 2023]]ï¼‰
        - [[@2023ScissorhandsExploitingPersistenceLiu|Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time, 2023]]
    - [[@2024CItruSChunkedInstructionawareBai|CItruS: Chunked Instruction-aware State Eviction for Long Sequence Modeling, 2024]]
        - è¯­è¨€å»ºæ¨¡ä¸ä»»åŠ¡è§£å†³åˆ†å¼€å‹ç¼©
    - attention sink
        - [[@2023EfficientStreamingLanguageXiao|Efficient Streaming Language Models with Attention Sinks, 2023]]
            - æ¨ç†æ—¶åªä¿ç•™å‰ $n=4$ ä¸ª tokenï¼Œåé¢çš„ kvcache æ»šåŠ¨ï¼Œå¯ä»¥è§†ä¸º kvcache å‹ç¼©
        - [[@2024UnderstandingCollapseLLMsYang|Understanding the Collapse of LLMs in Model Editing, 2024]]
            - å¼€å¤´ token åˆ†å¸ƒå‘ˆç°æ˜¾è‘—ä¸åŒï¼Œæœ¬æ–‡å…³æ³¨çš„å½±å“ï¼šå¯¼è‡´çŸ¥è¯†ç¼–è¾‘æ–¹æ³•å¤±æ•ˆ
- [[@2024DynamicMemoryCompressionNawrot|Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference, 2024]]
- [[@2024SelectiveAttentionImprovesLeviathan|Selective Attention Improves Transformer, 2024]]
- [[@2024RetrievalHeadMechanisticallyWu|Retrieval Head Mechanistically Explains Long-Context Factuality, 2024]]
- [[@2024DuoAttentionEfficientLongContextXiao|DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads, 2024]]

åšå®¢ï¼š

- ç»¼è¿°ï¼š [RoPE å¤–æ¨çš„ç¼©æ”¾æ³•åˆ™ â€”â€” å°è¯•å¤–æ¨ RoPE è‡³ 1M ä¸Šä¸‹æ–‡ - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/660073229)
- è®¡ç®—å¤æ‚åº¦ï¼š [çº¿æ€§ Transformer åº”è¯¥ä¸æ˜¯ä½ è¦ç­‰çš„é‚£ä¸ªæ¨¡å‹ - ç§‘å­¦ç©ºé—´|Scientific Spaces](https://spaces.ac.cn/archives/8610)
- [Transformer å‡çº§ä¹‹è·¯ï¼š7ã€é•¿åº¦å¤–æ¨æ€§ä¸å±€éƒ¨æ³¨æ„åŠ› - ç§‘å­¦ç©ºé—´|Scientific Spaces](https://spaces.ac.cn/archives/9431)

PEï¼š

- [[@2023LengthExtrapolatableTransformerSun|xPos]] ï¼ˆ[[@2023LengthExtrapolatableTransformerSun|A Length-Extrapolatable Transformer, 2023]]ï¼‰
    - RoPE+ALiBi
- [[@2021RethinkingPositionalEncodingKe|Rethinking Positional Encoding in Language Pre-training, 2021]]
- [[@2024DAPEDataAdaptivePositionalZheng|DAPE: Data-Adaptive Positional Encoding for Length Extrapolation, 2024]]

NoPEï¼š

- [[@2023ImpactPositionalEncodingKazemnejad|The Impact of Positional Encoding on Length Generalization in Transformers, 2023]]
- [[@2022TransformerLanguageModelsHaviv|Transformer Language Models without Positional Encodings Still Learn Positional Information, 2022]]
- [[@2023LatentPositionalInformationChi|Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings, 2023]]
- [[@2024ExploringContextWindowDong|Exploring Context Window of Large Language Models via Decomposed Positional Vectors, 2024]]
    - é€šè¿‡å¹³å‡éšå±‚å¾—åˆ°éšå¼ä½ç½®å‘é‡ï¼ŒPCAé™ç»´å¯è§†åŒ–
    - æ–¹æ³•æ¥è‡ªï¼š[[@2024UncoveringHiddenGeometrySong|Uncovering hidden geometry in Transformers via disentangling position and context, 2024]]

RoPE+NoPE:
- DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model
- HoPE: A Novel Positional Encoding Without Long-Term Decay for Enhanced Context Awareness and Extrapolation


å·¥ç¨‹ï¼š

- [[@2023BlockwiseParallelTransformerLiu|Blockwise Parallel Transformer for Long Context Large Models, 2023]]
    - è¿™ç¯‡ä¹Ÿæ˜¯é’ˆå¯¹ memory çš„ç³»ç»Ÿå·¥ç¨‹æ–¹æ¡ˆã€‚å¤§æ¦‚æ€æƒ³åº”è¯¥å’Œ flash attention ä¸€æ ·ï¼ŒæŠŠ input åˆ†å—ï¼Œè¿™æ · attention å†…å­˜å¯ä»¥å‰©ã€‚è¿™é‡Œåº”è¯¥æ˜¯è¯´ feed forward ä¹Ÿå¯ä»¥ä»åˆ†å—è¿‡ç¨‹ä¸­å‰©å†…å­˜(ä½†çœ‹ä»–ä»¬ä»£ç å¥½åƒä¹Ÿæ²¡æœ‰äº› CUDAï¼Œç›´æ¥ç”¨ jax å®ç°ã€‚ã€‚)

[å‹ç¼©ä½ çš„ Promptï¼Œè®© LLMs å¤„ç†å¤šè¾¾ 2 å€çš„ Context - çŸ¥ä¹ (zhihu.com)](https://zhuanlan.zhihu.com/p/625440016?utm_medium=social&utm_oi=30536802238464&utm_psn=1635194469631832064&utm_source=wechat_timeline&utm_id=0)

ç»“åˆ in-contextï¼š

- [[@2022StructuredPromptingScalingHao|Structured Prompting: Scaling In-Context Learning to 1,000 Examples, 2022]]

ç»¼è¿°ï¼š

- [[@2022EfficientTransformersSurveyTay|Efficient Transformers: A Survey, 2022]]
- [[@2023SurveyLongTextDong|A Survey on Long Text Modeling with Transformers, 2023]]

å¦‚ä½•å®ç°æ›´é•¿çš„è¾“å‡ºï¼š

| æ–¹æ³•                       | ä»£è¡¨å·¥ä½œ                       |                            è®¡ç®—é‡                            | ä½¿ç”¨éš¾åº¦ | ä¼˜ç‚¹                         | ç¼ºç‚¹                                                                      | æ½œåœ¨é•¿åº¦ä¸Šé™                                     |
| :------------------------- | :----------------------------- | :----------------------------------------------------------: | :------- | :--------------------------- | :------------------------------------------------------------------------ | :----------------------------------------------- |
| åŸºäºçŠ¶æ€è¿­ä»£çš„æ–¹æ³•         | TransformerXL,RMT              |                           $O(n^2)$                           | ç®€å•     | è®¡ç®—é‡éšç€è¾“å…¥é•¿åº¦å¢åŠ ä¸æ˜æ˜¾ | æ²¡æœ‰ç»è¿‡å¤§æ¨¡å‹é¢„è®­ç»ƒéªŒè¯                                                  | 1 million(?)                                     |
| åŸºäºä½ç½®ç¼–ç å¤–æ¨èƒ½åŠ›çš„æ–¹æ³• | ALiBi<br>xPos<br>Unlimiformer  |                       $O(\frac NLL^2)$                       | ç®€å•     | è®­ç»ƒæ—¶ä»£ä»·å°                 | -                                                                         | MPT æ¨¡å‹éªŒè¯äº† ALiBi åœ¨ 64K è®­ç»ƒï¼Œå¯ä»¥å¤–æ¨åˆ° 84K |
| åŸºäºå·¥ç¨‹ä¼˜åŒ–çš„æ–¹æ³•         | FlashAttention                 |                           $O(n^2)$                           | ç®€å•     | å®Œå…¨æ— æŸçš„ attention æ›¿æ¢    | åªèƒ½åœ¨ç‰¹å®šç»´åº¦ä¸Šä½¿ç”¨ï¼Œä¸è¿‡è¿™äº›ç‰¹å®šç»´åº¦ç›®å‰ä¹Ÿæ»¡è¶³éœ€æ±‚                      | 128k                                             |
| åŸºäºé«˜æ•ˆ Attention ç®—æ³•    | Reformer<br>Linformer<br>Flash | $\begin{array}{c}O(n) \\\downarrow \\O(n \log n)\end{array}$ | ç®€å•     | è®¡ç®—é‡å°ã€çœæ˜¾å­˜             | æ¯”è¾ƒç¼ºä¹é«˜æ•ˆç®—å­ã€å¤§éƒ¨åˆ†éƒ½é’ˆå¯¹ Encoder è¿›è¡Œä¼˜åŒ–ã€æ²¡æœ‰ç»è¿‡å¤§æ¨¡å‹é¢„è®­ç»ƒéªŒè¯ | -                                                |
| å…¶ä»–æ–¹æ³•                   | S4<br>FLASH                    |                              -                               | -        | è®¡ç®—é‡å°ã€çœæ˜¾å­˜             | ç¼ºä¹é«˜æ•ˆç®—å­ã€ç¼ºä¹å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒéªŒè¯                                      | -                                                |

> from ADL138 5.26 åŸºç¡€æ¨¡å‹çš„åˆ›æ–°ç½‘ç»œæ¶æ„ é¢œèˆª

evalï¼š

- language modeling (PPL)ï¼š
    - å’Œ long ctx æ²¡æœ‰ç›´æ¥å…³è”
        - ç›¸è·è¶Šè¿œçš„ tokenï¼Œç›¸å…³æ€§è¶Šå¼±ï¼ˆ [Transformer å‡çº§ä¹‹è·¯ï¼š1ã€Sinusoidal ä½ç½®ç¼–ç è¿½æ ¹æº¯æº - ç§‘å­¦ç©ºé—´|Scientific Spaces](https://spaces.ac.cn/archives/8231#%E8%BF%9C%E7%A8%8B%E8%A1%B0%E5%87%8F) ï¼‰
    - ä½œä¸ºåŸºç¡€æµ‹è¯•ï¼Œæµ‹è¯•æ¨¡å‹æ˜¯å¦å´©æºƒã€‚
    - [[@2022TrainShortTestPress|ALiBi]]
    - [[@2024CanPerplexityReflectHu|Can Perplexity Reflect Large Language Model's Ability in Long Text Understanding?, 2024]]
- synthetic retrieval tasksï¼šåŸºäº LLM å¯¹è¯èƒ½åŠ›
    - Passkey Retrievalï¼ˆ[[@2023LandmarkAttentionRandomAccessMohtashami|Landmark Attention: Random-Access Infinite Context Length for Transformers]]ï¼Œ[[@2023ExtendingContextWindowChen|Extending Context Window of Large Language Models via Positional Interpolation]]ï¼‰
    - [[@2024CountingStarsMultievidencePositionawareSong|Counting-Stars: A Multi-evidence, Position-aware, and Scalable Benchmark for Evaluating Long-Context Large Language Models, 2024]]
    - LongEvalï¼ˆ [How Long Can Open-Source LLMs Truly Promise on Context Length? | LMSYS Org](https://lmsys.org/blog/2023-06-29-longchat/#evaluation-toolkits-longeval) ï¼‰
    - StreamEvalï¼šLongEval Streaming versionï¼ˆ[[@2023EfficientStreamingLanguageXiao|StreamingLLM]]ï¼‰
    - [NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)
        - Claude çš„å›å‡»ï¼š[Long context prompting for Claude 2.1 \ Anthropic](https://www.anthropic.com/news/claude-2-1-prompting)
- synthetic reasoning tasksï¼š
    - Long ListOpsï¼ˆ[[@2020LongRangeArenaTay|LRA]]ï¼‰
    - [[@2023NeuralNetworksChomskyDeletang|Neural Networks and the Chomsky Hierarchy]]ï¼šéœ€è¦ä»é›¶å¼€å§‹è®­ç»ƒ
- real-world long-context tasks
    - Summarization
        - GovReportï¼ˆ[[@2023ExtendingContextWindowChen|Extending Context Window of Large Language Models via Positional Interpolation]]ï¼‰
    - QA
        - TriviaQAï¼ˆ[[@2022SimpleLocalAttentionsXiong|Simple Local Attentions Remain Competitive for Long-Context Tasks]]ï¼‰
        - QasperQAï¼ˆ [How Long Can Open-Source LLMs Truly Promise on Context Length? | LMSYS Org](https://lmsys.org/blog/2023-06-29-longchat/#long-sequence-question-answer-benchmark) ï¼‰
    - classification
        - Hyperpartisanï¼ˆ[[@2022SimpleLocalAttentionsXiong|Simple Local Attentions Remain Competitive for Long-Context Tasks]]ï¼‰
    - MT-benchï¼ˆ [How Long Can Open-Source LLMs Truly Promise on Context Length? | LMSYS Org](https://lmsys.org/blog/2023-06-29-longchat/#human-preference-benchmark-mt-bench) ï¼‰
- Few-shot in-context learningï¼ˆ[[@2023FocusedTransformerContrastiveTworkowski|Focused Transformer: Contrastive Training for Context Scaling]]
- Mixï¼š[[@2023LongBenchBilingualMultitaskBai|LongBench]] by THUDMï¼ˆ [github](https://github.com/THUDM/LongBench) ï¼‰
