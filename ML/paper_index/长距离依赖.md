---
~
---

相关论文收集：[Xnhyacinth/Awesome-LLM-Long-Context-Modeling: 📰 Must-read papers and blogs on LLM based Long Context Modeling 🔥](https://github.com/Xnhyacinth/Awesome-LLM-Long-Context-Modeling#5-length-extrapolation)

[[@2023EfficientStreamingLanguageXiao#相关研究|Efficient Streaming Language Models with Attention Sinks, 2023]] 提出了 Long Context 的三个方向：长度外推（Length Extrapolation）、扩展（预训练）上下文窗口（Context Window Extension）、有效利用长文本（Improving LLMs’ Utilization of Long Text）

**长度外推（Length Extrapolation）：**

- [[@2022TrainShortTestPress|ALiBi]]（[[@2022TrainShortTestPress|Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation, 2022]]）
    - 一种新的 attention 计算方式。
    - 采用的设定是不微调的外推。

why Extrapolation doesn't work:

- Transformers overfit to positions（ [ALiBi enables transformer language models to handle longer inputs - YouTube](https://youtu.be/Pp61ShI9VGc?t=1266) ），用到了没训练过的位置编码
- 预测的时候注意力机制所处理的 token 数量远超训练时的数量（ [Transformer 升级之路：7、长度外推性与局部注意力 - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/9431#%E8%B6%85%E5%BC%BA%E5%9F%BA%E7%BA%BF) ）。应对：乘 $\log n$ （ [从熵不变性看 Attention 的 Scale 操作 - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/8823) ）

**扩展（预训练）上下文窗口（Context Window Extension）：**

- 工程优化：
    - [[@2022FlashAttentionFastMemoryEfficientDao|FlashAttention]]
    - [[@2023RingAttentionBlockwiseLiua|Ring Attention with Blockwise Transformers for Near-Infinite Context, 2023]]
- RNN 相关的方法：
    - [[@2019TransformerXLAttentiveLanguageDai|Transformer-XL]]
    - [[@2023ScalingTransformer1MBulatov|Scaling Transformer to 1M tokens and beyond with RMT, 2023]]
- 高效（近似 / 稀疏） Attention：
    - [[@2020LongformerLongDocumentTransformerBeltagy|Longformer]]：sparse attention
    - BigBird
    - [[@2022RethinkingAttentionPerformersChoromanski|Performer]]
    - [[@2023LongNetScalingTransformersDing|LongNet: Scaling Transformers to 1,000,000,000 Tokens, 2023]]
        - dilated attention
    - [[@2022SimpleLocalAttentionsXiong|Simple Local Attentions Remain Competitive for Long-Context Tasks, 2022]]
    - (Sliding/Blockwise) Window Attention 从头预训练
- [[@2023ExtendingContextWindowChen|RoPE-PI]]（[[@2023ExtendingContextWindowChen|Extending Context Window of Large Language Models via Positional Interpolation, 2023]]）：通过位置 id 的内插+微调实现更长的上下文长度
    - [[@2024LongLoRAEfficientFinetuningChen|LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models, 2024]]
        - 降低 PI 训练成本：PI + 分块注意力(S2Attn) + PEFT(LoRA+Norm+Embed)
    - [[@2024LLMMaybeLongLMJin|SelfExtend]]（[[@2024LLMMaybeLongLMJin|LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning, 2024]]）
        - 近距离精确 ID+远距离内插 ID
        - [Transformer 升级之路：12、无限外推的 ReRoPE？ - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/9708)
- [[@2023EffectiveLongContextScalingXiong|ABF]] (adjusted base frequency)（[[@2023EffectiveLongContextScalingXiong|Effective Long-Context Scaling of Foundation Models, 2023]]）
    - [[@2024ExtendingLLMsContextZhanga|Extending LLMs' Context Window with 100 Samples, 2024]]
    - 解释：
        - [[@2024ScalingLawsRoPEbasedLiu|Scaling Laws of RoPE-based Extrapolation, 2024]]
        - [[@2024BaseRoPEBoundsMen|Base of RoPE Bounds Context Length, 2024]]
- [[@2023YaRNEfficientContextPeng|YaRN]]
    - [Transformer 升级之路：10、RoPE 是一种 β 进制编码 - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/9675)
- [[@2024LongRoPEExtendingLLMDing|LongRoPE]]（[[@2024LongRoPEExtendingLLMDing|LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens, 2024]]）
    - PI/ABF 调参

**有效利用长文本（Improving LLMs’ Utilization of Long Text）：**

- 暂无

RAG:

- [[@2023UnlimiformerLongRangeTransformersBertsch|Unlimiformer: Long-Range Transformers with Unlimited Length Input, 2023]]
- [[@2023FocusedTransformerContrastiveTworkowski|LongLLaMA]]（[[@2023FocusedTransformerContrastiveTworkowski|Focused Transformer: Contrastive Training for Context Scaling, 2023]]）
    - 对比学习+RAG

New model:

- SSM (structured state space models): [[@2023MambaLinearTimeSequenceGu|Mamba]]

TODO:

- [[@2023LearningCompressPromptsMu|Learning to Compress Prompts with Gist Tokens, 2023]]
    - soft
- [[@2023DissectingTransformerLengthChi|Sandwich]]（[[@2023DissectingTransformerLengthChi|Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis, 2023]]）
- [[@2022ExploringLengthGeneralizationAnil|Exploring Length Generalization in Large Language Models, 2022]]
- [[@2023LandmarkAttentionRandomAccessMohtashami|Landmark Attention: Random-Access Infinite Context Length for Transformers, 2023]]
- [[@2023PoSEEfficientContextZhu|PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training, 2023]]
    - 扩展 PE（2048 -> 4096），和 [[@2023ExtendingContextWindowChen|interpolation]] 一样需要 fine tune 整个模型。但 fine tune 的时候不需要把窗口扩展（保持 2048 不变），把中间去掉一些 position（skipped position）。
- RWKV: Capturing Long-range Dependencies in RWKV
- [[@2023LMInfiniteSimpleOntheFlyHan|LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models]]
    - focuses on "on-thefly" length generalization for non-fine-tuned models.
    - 提出 λ 形状的 attn mask
    - 有长度和熵方面的理论分析
- [[@2023M4LEMultiAbilityMultiRangeKwan|M4LE: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models, 2023]]
- [[@2024LeaveNoContextMunkhdalai|Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention, 2024]]
    - 压缩
- [[@2023AugmentingLanguageModelsWang|Augmenting Language Models with Long-Term Memory, 2023]]
    - side tuning （[[@2022LSTLadderSideTuningSung|LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning, 2022]]）
- [[@2024MegalodonEfficientLLMMa|Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length, 2024]]
    - 归一化注意力
- [[@2024MitigatePositionBiasYu|Mitigate Position Bias in Large Language Models via Scaling a Single Dimension, 2024]]
    - 解释性工作，隐层某一维的操作，casual masking 带来 bias
- Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon
- Extending Context Window of Large Language Models from a Distributional Perspective
- Evaluation of similarity-based explanations

[iclr24]In-Context Pretraining: Language Modeling Beyond Document Boundaries
[iclr24]Retrieval meets Long Context Large Language Models
[iclr24]What Algorithms can Transformers Learn? A Study in Length Generalization
[iclr24]Functional Interpolation for Relative Positions improves Long Context Transformers
[iclr24]On the Limitations of Temperature Scaling for Distributions with Overlaps, 这篇是说 TS 是一种解决 overconfidence 的 calibration 方法，但也有缺点。Head Scale 在 length generalization 上有效是否可以从这个角度再看看？
[iclr24]Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs, 如果后续要继续操作 attention head,是否可以做类似的操作

数据：

- [[@2024DataEngineeringScalingFu|Data Engineering for Scaling Language Models to 128K Context, 2024]]
- [[@2024LongContextNotChen|Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models, 2024]]
- Selecting Influential Samples for Long Context Alignment via Homologous Models' Guidance and Contextual Awareness Measurement

- 注意力模式分析：
    - 推理时 kv-cache 压缩：
    - [[@2024ModelTellsYouGe|Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs, 2024]]
        - 分析 attention head 的模式，针对一些固定模式可以进行 KV cache 的压缩，从而加速推理
        - 两篇类似方法：
        - [[@2023_2HeavyHitterOracleZhang|Heavy Hitter]]（[[@2023_2HeavyHitterOracleZhang|H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models, 2023]]）
        - [[@2023ScissorhandsExploitingPersistenceLiu|Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time, 2023]]
    - [[@2024CItruSChunkedInstructionawareBai|CItruS: Chunked Instruction-aware State Eviction for Long Sequence Modeling, 2024]]
        - 语言建模与任务解决分开压缩
    - attention sink
        - [[@2023EfficientStreamingLanguageXiao|Efficient Streaming Language Models with Attention Sinks, 2023]]
            - 推理时只保留前 $n=4$ 个 token，后面的 kvcache 滚动，可以视为 kvcache 压缩
        - [[@2024UnderstandingCollapseLLMsYang|Understanding the Collapse of LLMs in Model Editing, 2024]]
            - 开头 token 分布呈现显著不同，本文关注的影响：导致知识编辑方法失效
- [[@2024DynamicMemoryCompressionNawrot|Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference, 2024]]
- [[@2024SelectiveAttentionImprovesLeviathan|Selective Attention Improves Transformer, 2024]]
- [[@2024RetrievalHeadMechanisticallyWu|Retrieval Head Mechanistically Explains Long-Context Factuality, 2024]]
- [[@2024DuoAttentionEfficientLongContextXiao|DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads, 2024]]

博客：

- 综述： [RoPE 外推的缩放法则 —— 尝试外推 RoPE 至 1M 上下文 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/660073229)
- 计算复杂度： [线性 Transformer 应该不是你要等的那个模型 - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/8610)
- [Transformer 升级之路：7、长度外推性与局部注意力 - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/9431)

PE：

- [[@2023LengthExtrapolatableTransformerSun|xPos]] （[[@2023LengthExtrapolatableTransformerSun|A Length-Extrapolatable Transformer, 2023]]）
    - RoPE+ALiBi
- [[@2021RethinkingPositionalEncodingKe|Rethinking Positional Encoding in Language Pre-training, 2021]]
- [[@2024DAPEDataAdaptivePositionalZheng|DAPE: Data-Adaptive Positional Encoding for Length Extrapolation, 2024]]

NoPE：

- [[@2023ImpactPositionalEncodingKazemnejad|The Impact of Positional Encoding on Length Generalization in Transformers, 2023]]
- [[@2022TransformerLanguageModelsHaviv|Transformer Language Models without Positional Encodings Still Learn Positional Information, 2022]]
- [[@2023LatentPositionalInformationChi|Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings, 2023]]
- [[@2024ExploringContextWindowDong|Exploring Context Window of Large Language Models via Decomposed Positional Vectors, 2024]]
    - 通过平均隐层得到隐式位置向量，PCA降维可视化
    - 方法来自：[[@2024UncoveringHiddenGeometrySong|Uncovering hidden geometry in Transformers via disentangling position and context, 2024]]

RoPE+NoPE:
- DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model
- HoPE: A Novel Positional Encoding Without Long-Term Decay for Enhanced Context Awareness and Extrapolation


工程：

- [[@2023BlockwiseParallelTransformerLiu|Blockwise Parallel Transformer for Long Context Large Models, 2023]]
    - 这篇也是针对 memory 的系统工程方案。大概思想应该和 flash attention 一样，把 input 分块，这样 attention 内存可以剩。这里应该是说 feed forward 也可以从分块过程中剩内存(但看他们代码好像也没有些 CUDA，直接用 jax 实现。。)

[压缩你的 Prompt，让 LLMs 处理多达 2 倍的 Context - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/625440016?utm_medium=social&utm_oi=30536802238464&utm_psn=1635194469631832064&utm_source=wechat_timeline&utm_id=0)

结合 in-context：

- [[@2022StructuredPromptingScalingHao|Structured Prompting: Scaling In-Context Learning to 1,000 Examples, 2022]]

综述：

- [[@2022EfficientTransformersSurveyTay|Efficient Transformers: A Survey, 2022]]
- [[@2023SurveyLongTextDong|A Survey on Long Text Modeling with Transformers, 2023]]

如何实现更长的输出：

| 方法                       | 代表工作                       |                            计算量                            | 使用难度 | 优点                         | 缺点                                                                      | 潜在长度上限                                     |
| :------------------------- | :----------------------------- | :----------------------------------------------------------: | :------- | :--------------------------- | :------------------------------------------------------------------------ | :----------------------------------------------- |
| 基于状态迭代的方法         | TransformerXL,RMT              |                           $O(n^2)$                           | 简单     | 计算量随着输入长度增加不明显 | 没有经过大模型预训练验证                                                  | 1 million(?)                                     |
| 基于位置编码外推能力的方法 | ALiBi<br>xPos<br>Unlimiformer  |                       $O(\frac NLL^2)$                       | 简单     | 训练时代价小                 | -                                                                         | MPT 模型验证了 ALiBi 在 64K 训练，可以外推到 84K |
| 基于工程优化的方法         | FlashAttention                 |                           $O(n^2)$                           | 简单     | 完全无损的 attention 替换    | 只能在特定维度上使用，不过这些特定维度目前也满足需求                      | 128k                                             |
| 基于高效 Attention 算法    | Reformer<br>Linformer<br>Flash | $\begin{array}{c}O(n) \\\downarrow \\O(n \log n)\end{array}$ | 简单     | 计算量小、省显存             | 比较缺乏高效算子、大部分都针对 Encoder 进行优化、没有经过大模型预训练验证 | -                                                |
| 其他方法                   | S4<br>FLASH                    |                              -                               | -        | 计算量小、省显存             | 缺乏高效算子、缺乏大规模模型训练验证                                      | -                                                |

> from ADL138 5.26 基础模型的创新网络架构 颜航

eval：

- language modeling (PPL)：
    - 和 long ctx 没有直接关联
        - 相距越远的 token，相关性越弱（ [Transformer 升级之路：1、Sinusoidal 位置编码追根溯源 - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/8231#%E8%BF%9C%E7%A8%8B%E8%A1%B0%E5%87%8F) ）
    - 作为基础测试，测试模型是否崩溃。
    - [[@2022TrainShortTestPress|ALiBi]]
    - [[@2024CanPerplexityReflectHu|Can Perplexity Reflect Large Language Model's Ability in Long Text Understanding?, 2024]]
- synthetic retrieval tasks：基于 LLM 对话能力
    - Passkey Retrieval（[[@2023LandmarkAttentionRandomAccessMohtashami|Landmark Attention: Random-Access Infinite Context Length for Transformers]]，[[@2023ExtendingContextWindowChen|Extending Context Window of Large Language Models via Positional Interpolation]]）
    - [[@2024CountingStarsMultievidencePositionawareSong|Counting-Stars: A Multi-evidence, Position-aware, and Scalable Benchmark for Evaluating Long-Context Large Language Models, 2024]]
    - LongEval（ [How Long Can Open-Source LLMs Truly Promise on Context Length? | LMSYS Org](https://lmsys.org/blog/2023-06-29-longchat/#evaluation-toolkits-longeval) ）
    - StreamEval：LongEval Streaming version（[[@2023EfficientStreamingLanguageXiao|StreamingLLM]]）
    - [NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)
        - Claude 的回击：[Long context prompting for Claude 2.1 \ Anthropic](https://www.anthropic.com/news/claude-2-1-prompting)
- synthetic reasoning tasks：
    - Long ListOps（[[@2020LongRangeArenaTay|LRA]]）
    - [[@2023NeuralNetworksChomskyDeletang|Neural Networks and the Chomsky Hierarchy]]：需要从零开始训练
- real-world long-context tasks
    - Summarization
        - GovReport（[[@2023ExtendingContextWindowChen|Extending Context Window of Large Language Models via Positional Interpolation]]）
    - QA
        - TriviaQA（[[@2022SimpleLocalAttentionsXiong|Simple Local Attentions Remain Competitive for Long-Context Tasks]]）
        - QasperQA（ [How Long Can Open-Source LLMs Truly Promise on Context Length? | LMSYS Org](https://lmsys.org/blog/2023-06-29-longchat/#long-sequence-question-answer-benchmark) ）
    - classification
        - Hyperpartisan（[[@2022SimpleLocalAttentionsXiong|Simple Local Attentions Remain Competitive for Long-Context Tasks]]）
    - MT-bench（ [How Long Can Open-Source LLMs Truly Promise on Context Length? | LMSYS Org](https://lmsys.org/blog/2023-06-29-longchat/#human-preference-benchmark-mt-bench) ）
- Few-shot in-context learning（[[@2023FocusedTransformerContrastiveTworkowski|Focused Transformer: Contrastive Training for Context Scaling]]
- Mix：[[@2023LongBenchBilingualMultitaskBai|LongBench]] by THUDM（ [github](https://github.com/THUDM/LongBench) ）
