---
~
---

[[@2023EfficientStreamingLanguageXiao#相关研究|Efficient Streaming Language Models with Attention Sinks, 2023]] 提出了 Long Context 的三个方向：长度外推（Length Extrapolation）、扩展（预训练）上下文窗口（Context Window Extension）、有效利用长文本（Improving LLMs’ Utilization of Long Text）

**长度外推（Length Extrapolation）：**

-   [[@2022TrainShortTestPress|ALiBi]]（[[@2022TrainShortTestPress|Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation, 2022]]）
    -   一种新的 attention 计算方式。
    -   采用的设定是不微调的外推。

why Extrapolation doesn't work:

-   Transformers overfit to positions（ [ALiBi enables transformer language models to handle longer inputs - YouTube](https://youtu.be/Pp61ShI9VGc?t=1266) ），用到了没训练过的位置编码
-   预测的时候注意力机制所处理的 token 数量远超训练时的数量（ [Transformer 升级之路：7、长度外推性与局部注意力 - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/9431#%E8%B6%85%E5%BC%BA%E5%9F%BA%E7%BA%BF) ）。应对：乘 $\log n$ （ [从熵不变性看 Attention 的 Scale 操作 - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/8823) ）

**扩展（预训练）上下文窗口（Context Window Extension）：**

-   工程优化：
    -   [[@2022FlashAttentionFastMemoryEfficientDao|FlashAttention]]
    -   [[@2023RingAttentionBlockwiseLiua|Ring Attention with Blockwise Transformers for Near-Infinite Context, 2023]]
-   RNN 相关的方法：
    -   [[@2019TransformerXLAttentiveLanguageDai|Transformer-XL]]
    -   [[@2023ScalingTransformer1MBulatov|Scaling Transformer to 1M tokens and beyond with RMT, 2023]]
-   高效（近似 / 稀疏） Attention：
    -   [[@2020LongformerLongDocumentTransformerBeltagy|Longformer]]：sparse attention
    -   BigBird
    -   [[@2022RethinkingAttentionPerformersChoromanski|Performer]]
    -   [[@2023LongNetScalingTransformersDing|LongNet: Scaling Transformers to 1,000,000,000 Tokens, 2023]]
        -   dilated attention
    -   [[@2022SimpleLocalAttentionsXiong|Simple Local Attentions Remain Competitive for Long-Context Tasks, 2022]]
    -   (Sliding/Blockwise) Window Attention 从头预训练
-   [[@2023ExtendingContextWindowChen|RoPE-PI]]（[[@2023ExtendingContextWindowChen|Extending Context Window of Large Language Models via Positional Interpolation, 2023]]）：通过位置 id 的内插+微调实现更长的上下文长度
    -   [[@2024LongLoRAEfficientFinetuningChen|LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models, 2024]]
        -   降低 PI 训练成本：PI + 分块注意力(S2Attn) + PEFT(LoRA+Norm+Embed)
    -   [[@2024LLMMaybeLongLMJin|SelfExtend]]（[[@2024LLMMaybeLongLMJin|LLM Maybe LongLM: Self-Extend LLM Context Window Without Tuning, 2024]]）
        -   近距离精确 ID+远距离内插 ID
        -   [Transformer 升级之路：12、无限外推的 ReRoPE？ - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/9708)
-   [[@2023EffectiveLongContextScalingXiong|ABF]] (adjusted base frequency)（[[@2023EffectiveLongContextScalingXiong|Effective Long-Context Scaling of Foundation Models, 2023]]）
    -   [[@2024ExtendingLLMsContextZhanga|Extending LLMs' Context Window with 100 Samples, 2024]]
    -   解释：
        -   [[@2024ScalingLawsRoPEbasedLiu|Scaling Laws of RoPE-based Extrapolation, 2024]]
        -   [[@2024BaseRoPEBoundsMen|Base of RoPE Bounds Context Length, 2024]]
-   [[@2023YaRNEfficientContextPeng|YaRN]]
    -   [Transformer 升级之路：10、RoPE 是一种 β 进制编码 - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/9675)
-   [[@2024LongRoPEExtendingLLMDing|LongRoPE]]（[[@2024LongRoPEExtendingLLMDing|LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens, 2024]]）
    -   PI/ABF 调参

**有效利用长文本（Improving LLMs’ Utilization of Long Text）：**

-   暂无

RAG:

-   [[@2023UnlimiformerLongRangeTransformersBertsch|Unlimiformer: Long-Range Transformers with Unlimited Length Input, 2023]]
-   [[@2023FocusedTransformerContrastiveTworkowski|LongLLaMA]]（[[@2023FocusedTransformerContrastiveTworkowski|Focused Transformer: Contrastive Training for Context Scaling, 2023]]）
    -   对比学习+RAG

New model:

-   SSM (structured state space models): [[@2023MambaLinearTimeSequenceGu|Mamba]]

TODO:

-   [[@2023LearningCompressPromptsMu|Learning to Compress Prompts with Gist Tokens, 2023]]
    -   soft
-   [[@2023DissectingTransformerLengthChi|Sandwich]]（[[@2023DissectingTransformerLengthChi|Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis, 2023]]）
-   [[@2022ExploringLengthGeneralizationAnil|Exploring Length Generalization in Large Language Models, 2022]]
-   [[@2023LandmarkAttentionRandomAccessMohtashami|Landmark Attention: Random-Access Infinite Context Length for Transformers, 2023]]
-   [[@2023PoSEEfficientContextZhu|PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training, 2023]]
    -   扩展 PE（2048 -> 4096），和 [[@2023ExtendingContextWindowChen|interpolation]] 一样需要 fine tune 整个模型。但 fine tune 的时候不需要把窗口扩展（保持 2048 不变），把中间去掉一些 position（skipped position）。
-   [[@2023EfficientStreamingLanguageXiao|Efficient Streaming Language Models with Attention Sinks, 2023]]
-   RWKV: Capturing Long-range Dependencies in RWKV
-   [[@2023LMInfiniteSimpleOntheFlyHan|LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models]]
    -   focuses on "on-thefly" length generalization for non-fine-tuned models.
    -   提出 λ 形状的 attn mask
    -   有长度和熵方面的理论分析
-   [[@2023M4LEMultiAbilityMultiRangeKwan|M4LE: A Multi-Ability Multi-Range Multi-Task Multi-Domain Long-Context Evaluation Benchmark for Large Language Models, 2023]]
-   [[@2024LeaveNoContextMunkhdalai|Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention, 2024]]
    -   压缩
-   [[@2023AugmentingLanguageModelsWang|Augmenting Language Models with Long-Term Memory, 2023]]
    -   side tuning （[[@2022LSTLadderSideTuningSung|LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning, 2022]]）
-   [[@2024MegalodonEfficientLLMMa|Megalodon: Efficient LLM Pretraining and Inference with Unlimited Context Length, 2024]]
    -   归一化注意力
-   [[@2024MitigatePositionBiasYu|Mitigate Position Bias in Large Language Models via Scaling a Single Dimension, 2024]]
    -   解释性工作，隐层某一维的操作，casual masking 带来 bias
-   Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon
-   Extending Context Window of Large Language Models from a Distributional Perspective
-   Evaluation of similarity-based explanations

[iclr24]In-Context Pretraining: Language Modeling Beyond Document Boundaries
[iclr24]Retrieval meets Long Context Large Language Models
[iclr24]What Algorithms can Transformers Learn? A Study in Length Generalization
[iclr24]Functional Interpolation for Relative Positions improves Long Context Transformers
[iclr24]On the Limitations of Temperature Scaling for Distributions with Overlaps, 这篇是说 TS 是一种解决 overconfidence 的 calibration 方法，但也有缺点。Head Scale 在 length generalization 上有效是否可以从这个角度再看看？
[iclr24]Tell Your Model Where to Attend: Post-hoc Attention Steering for LLMs, 如果后续要继续操作 attention head,是否可以做类似的操作

数据：

-   [[@2024DataEngineeringScalingFu|Data Engineering for Scaling Language Models to 128K Context, 2024]]
-   [[@2024LongContextNotChen|Long Context is Not Long at All: A Prospector of Long-Dependency Data for Large Language Models, 2024]]
-   Selecting Influential Samples for Long Context Alignment via Homologous Models' Guidance and Contextual Awareness Measurement

-   注意力模式分析：
    -   推理时 kv-cache 压缩：
    -   [[@2024ModelTellsYouGe|Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs, 2024]]
        -   分析 attention head 的模式，针对一些固定模式可以进行 KV cache 的压缩，从而加速推理
        -   两篇类似方法：
        -   [[@2023_2HeavyHitterOracleZhang|Heavy Hitter]]（[[@2023_2HeavyHitterOracleZhang|H$_2$O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models, 2023]]）
        -   [[@2023ScissorhandsExploitingPersistenceLiu|Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time, 2023]]
-   [[@2024DynamicMemoryCompressionNawrot|Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference, 2024]]
-   [[@2024SelectiveAttentionImprovesLeviathan|Selective Attention Improves Transformer, 2024]]
-   [[@2024RetrievalHeadMechanisticallyWu|Retrieval Head Mechanistically Explains Long-Context Factuality, 2024]]
-   [[@2024DuoAttentionEfficientLongContextXiao|DuoAttention: Efficient Long-Context LLM Inference with Retrieval and Streaming Heads, 2024]]

博客：

-   综述： [RoPE 外推的缩放法则 —— 尝试外推 RoPE 至 1M 上下文 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/660073229)
-   计算复杂度： [线性 Transformer 应该不是你要等的那个模型 - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/8610)

PE：

-   [[@2023LengthExtrapolatableTransformerSun|xPos]] （[[@2023LengthExtrapolatableTransformerSun|A Length-Extrapolatable Transformer, 2023]]）
    -   RoPE+ALiBi
-   [[@2021RethinkingPositionalEncodingKe|Rethinking Positional Encoding in Language Pre-training, 2021]]
-   [[@2024DAPEDataAdaptivePositionalZheng|DAPE: Data-Adaptive Positional Encoding for Length Extrapolation, 2024]]

NoPE：

-   [[@2023ImpactPositionalEncodingKazemnejad|The Impact of Positional Encoding on Length Generalization in Transformers, 2023]]
-   [[@2022TransformerLanguageModelsHaviv|Transformer Language Models without Positional Encodings Still Learn Positional Information, 2022]]
-   [[@2023LatentPositionalInformationChi|Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings, 2023]]
-   [[@2024ExploringContextWindowDong|Exploring Context Window of Large Language Models via Decomposed Positional Vectors, 2024]]

工程：

-   [[@2023BlockwiseParallelTransformerLiu|Blockwise Parallel Transformer for Long Context Large Models, 2023]]
    -   这篇也是针对 memory 的系统工程方案。大概思想应该和 flash attention 一样，把 input 分块，这样 attention 内存可以剩。这里应该是说 feed forward 也可以从分块过程中剩内存(但看他们代码好像也没有些 CUDA，直接用 jax 实现。。)

[压缩你的 Prompt，让 LLMs 处理多达 2 倍的 Context - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/625440016?utm_medium=social&utm_oi=30536802238464&utm_psn=1635194469631832064&utm_source=wechat_timeline&utm_id=0)

结合 in-context：

-   [[@2022StructuredPromptingScalingHao|Structured Prompting: Scaling In-Context Learning to 1,000 Examples, 2022]]

综述：

-   [[@2022EfficientTransformersSurveyTay|Efficient Transformers: A Survey, 2022]]
-   [[@2023SurveyLongTextDong|A Survey on Long Text Modeling with Transformers, 2023]]

[Transformer 升级之路：7、长度外推性与局部注意力 - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/9431)

如何实现更长的输出：

| 方法                       | 代表工作                       |                            计算量                            | 使用难度 | 优点                         | 缺点                                                                      | 潜在长度上限                                     |
| :------------------------- | :----------------------------- | :----------------------------------------------------------: | :------- | :--------------------------- | :------------------------------------------------------------------------ | :----------------------------------------------- |
| 基于状态迭代的方法         | TransformerXL,RMT              |                           $O(n^2)$                           | 简单     | 计算量随着输入长度增加不明显 | 没有经过大模型预训练验证                                                  | 1 million(?)                                     |
| 基于位置编码外推能力的方法 | ALiBi<br>xPos<br>Unlimiformer  |                       $O(\frac NLL^2)$                       | 简单     | 训练时代价小                 | -                                                                         | MPT 模型验证了 ALiBi 在 64K 训练，可以外推到 84K |
| 基于工程优化的方法         | FlashAttention                 |                           $O(n^2)$                           | 简单     | 完全无损的 attention 替换    | 只能在特定维度上使用，不过这些特定维度目前也满足需求                      | 128k                                             |
| 基于高效 Attention 算法    | Reformer<br>Linformer<br>Flash | $\begin{array}{c}O(n) \\\downarrow \\O(n \log n)\end{array}$ | 简单     | 计算量小、省显存             | 比较缺乏高效算子、大部分都针对 Encoder 进行优化、没有经过大模型预训练验证 | -                                                |
| 其他方法                   | S4<br>FLASH                    |                              -                               | -        | 计算量小、省显存             | 缺乏高效算子、缺乏大规模模型训练验证                                      | -                                                |

> from ADL138 5.26 基础模型的创新网络架构 颜航

eval：

-   language modeling (PPL)：
    -   和 long ctx 没有直接关联
        -   相距越远的 token，相关性越弱（ [Transformer 升级之路：1、Sinusoidal 位置编码追根溯源 - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/8231#%E8%BF%9C%E7%A8%8B%E8%A1%B0%E5%87%8F) ）
    -   作为基础测试，测试模型是否崩溃。
    -   [[@2022TrainShortTestPress|ALiBi]]
    -   [[@2024CanPerplexityReflectHu|Can Perplexity Reflect Large Language Model's Ability in Long Text Understanding?, 2024]]
-   synthetic retrieval tasks：基于 LLM 对话能力
    -   Passkey Retrieval（[[@2023LandmarkAttentionRandomAccessMohtashami|Landmark Attention: Random-Access Infinite Context Length for Transformers]]，[[@2023ExtendingContextWindowChen|Extending Context Window of Large Language Models via Positional Interpolation]]）
    -   [[@2024CountingStarsMultievidencePositionawareSong|Counting-Stars: A Multi-evidence, Position-aware, and Scalable Benchmark for Evaluating Long-Context Large Language Models, 2024]]
    -   LongEval（ [How Long Can Open-Source LLMs Truly Promise on Context Length? | LMSYS Org](https://lmsys.org/blog/2023-06-29-longchat/#evaluation-toolkits-longeval) ）
    -   StreamEval：LongEval Streaming version（[[@2023EfficientStreamingLanguageXiao|StreamingLLM]]）
    -   [NeedleInAHaystack](https://github.com/gkamradt/LLMTest_NeedleInAHaystack)
        -   Claude 的回击：[Long context prompting for Claude 2.1 \ Anthropic](https://www.anthropic.com/news/claude-2-1-prompting)
-   synthetic reasoning tasks：
    -   Long ListOps（[[@2020LongRangeArenaTay|LRA]]）
    -   [[@2023NeuralNetworksChomskyDeletang|Neural Networks and the Chomsky Hierarchy]]：需要从零开始训练
-   real-world long-context tasks
    -   Summarization
        -   GovReport（[[@2023ExtendingContextWindowChen|Extending Context Window of Large Language Models via Positional Interpolation]]）
    -   QA
        -   TriviaQA（[[@2022SimpleLocalAttentionsXiong|Simple Local Attentions Remain Competitive for Long-Context Tasks]]）
        -   QasperQA（ [How Long Can Open-Source LLMs Truly Promise on Context Length? | LMSYS Org](https://lmsys.org/blog/2023-06-29-longchat/#long-sequence-question-answer-benchmark) ）
    -   classification
        -   Hyperpartisan（[[@2022SimpleLocalAttentionsXiong|Simple Local Attentions Remain Competitive for Long-Context Tasks]]）
    -   MT-bench（ [How Long Can Open-Source LLMs Truly Promise on Context Length? | LMSYS Org](https://lmsys.org/blog/2023-06-29-longchat/#human-preference-benchmark-mt-bench) ）
-   Few-shot in-context learning（[[@2023FocusedTransformerContrastiveTworkowski|Focused Transformer: Contrastive Training for Context Scaling]]
-   Mix：[[@2023LongBenchBilingualMultitaskBai|LongBench]] by THUDM（ [github](https://github.com/THUDM/LongBench) ）
