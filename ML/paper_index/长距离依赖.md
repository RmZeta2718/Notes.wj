long context：
- [[@2019TransformerXLAttentiveLanguageDai|Transformer-XL]]
- [[@2020LongformerLongDocumentTransformerBeltagy|Longformer]]
- BigBird
- [[@2022RethinkingAttentionPerformersChoromanski|Performer]]
- benchmark：
    - [[@2020LongRangeArenaTay|LRA]]
    - [[@2023NeuralNetworksChomskyDeletang|Neural Networks and the Chomsky Hierarchy, 2023]]
- [[@2023LearningCompressPromptsMu|Learning to Compress Prompts with Gist Tokens, 2023]]
- [[@2023ScalingTransformer1MBulatov|Scaling Transformer to 1M tokens and beyond with RMT, 2023]]
- [[@2023UnlimiformerLongRangeTransformersBertsch|Unlimiformer: Long-Range Transformers with Unlimited Length Input, 2023]]
- [[@2022TrainShortTestPress|Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation, 2022]]
- [[@2022SimpleLocalAttentionsXiong|Simple Local Attentions Remain Competitive for Long-Context Tasks, 2022]]

PE：
- [[@2021RethinkingPositionalEncodingKe|Rethinking Positional Encoding in Language Pre-training, 2021]]

工程：
- [[@2023BlockwiseParallelTransformerLiu|Blockwise Parallel Transformer for Long Context Large Models, 2023]]
    - 这篇也是针对memory的系统工程方案。大概思想应该和flash attention一样，把input分块，这样attention内存可以剩。这里应该是说feed forward也可以从分块过程中剩内存(但看他们代码好像也没有些CUDA，直接用jax实现。。)

[压缩你的Prompt，让LLMs处理多达2倍的Context - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/625440016?utm_medium=social&utm_oi=30536802238464&utm_psn=1635194469631832064&utm_source=wechat_timeline&utm_id=0)

结合in-context：
- [[@2022StructuredPromptingScalingHao|Structured Prompting: Scaling In-Context Learning to 1,000 Examples, 2022]]

综述：
- [[@2022EfficientTransformersSurveyTay|Efficient Transformers: A Survey, 2022]]

[Transformer升级之路：7、长度外推性与局部注意力 - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/9431)

如何实现更长的输出：

| 方法                       | 代表工作                       |                            计算量                            | 使用难度 | 优点                         | 缺点                                                                    | 潜在长度上限                               |
|:-------------------------- |:------------------------------ |:------------------------------------------------------------:|:-------- |:---------------------------- |:----------------------------------------------------------------------- |:------------------------------------------ |
| 基于状态迭代的方法         | TransformerXL,RMT              |                           $O(n^2)$                           | 简单     | 计算量随着输入长度增加不明显 | 没有经过大模型预训练验证                                                | 1 million(?)                               |
| 基于位置编码外推能力的方法 | ALiBi<br>xPos<br>Unlimiformer  |                       $O(\frac NLL^2)$                       | 简单     | 训练时代价小                 | -                                                                       | MPT模型验证了ALiBi在64K训练，可以外推到84K |
| 基于工程优化的方法         | FlashAttention                 |                           $O(n^2)$                           | 简单     | 完全无损的attention替换      | 只能在特定维度上使用，不过这些特定维度目前也满足需求                    | 128k                                       |
| 基于高效Attention算法      | Reformer<br>Linformer<br>Flash | $\begin{array}{c}O(n) \\\downarrow \\O(n \log n)\end{array}$ | 简单     | 计算量小、省显存             | 比较缺乏高效算子、大部分都针对Encoder进行优化、没有经过大模型预训练验证 | -                                          |
| 其他方法                   | S4<br>FLASH                    |                              -                               | -        | 计算量小、省显存             | 缺乏高效算子、缺乏大规模模型训练验证                                    | -                                          |

> from ADL138 5.26 基础模型的创新网络架构 颜航


