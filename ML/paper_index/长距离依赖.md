---
~
---

[[@2023EfficientStreamingLanguageXiao#相关研究|Efficient Streaming Language Models with Attention Sinks, 2023]] 提出了 Long Context 的三个方向：长度外推（Length Extrapolation）、扩展（预训练）上下文窗口（Context Window Extension）、有效利用长文本（Improving LLMs’ Utilization of Long Text）

**长度外推（Length Extrapolation）：**
- [[@2022TrainShortTestPress|ALiBi]]（[[@2022TrainShortTestPress|Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation, 2022]]）
    - 一种新的 attention 计算方式。
    - 采用的设定是不微调的外推。

why Extrapolation doesn't work:
- Transformers overfit to positions（ [ALiBi enables transformer language models to handle longer inputs - YouTube](https://youtu.be/Pp61ShI9VGc?t=1266) ），用到了没训练过的位置编码
- 预测的时候注意力机制所处理的 token 数量远超训练时的数量（ [Transformer 升级之路：7、长度外推性与局部注意力 - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/9431#%E8%B6%85%E5%BC%BA%E5%9F%BA%E7%BA%BF) ）。应对：乘 $\log n$ （ [从熵不变性看Attention的Scale操作 - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/8823) ）

**扩展（预训练）上下文窗口（Context Window Extension）：**
- 工程优化：
    - FlashAttention
- RNN 相关的方法：
    - [[@2019TransformerXLAttentiveLanguageDai|Transformer-XL]]
    - [[@2023ScalingTransformer1MBulatov|Scaling Transformer to 1M tokens and beyond with RMT, 2023]]
- 近似/稀疏 Attention：
    - [[@2020LongformerLongDocumentTransformerBeltagy|Longformer]]：sparse attention
    - BigBird
    - [[@2022RethinkingAttentionPerformersChoromanski|Performer]]
    - [[@2023LongNetScalingTransformersDing|LongNet: Scaling Transformers to 1,000,000,000 Tokens, 2023]]
        - dilated attention
    - [[@2022SimpleLocalAttentionsXiong|Simple Local Attentions Remain Competitive for Long-Context Tasks, 2022]]
        - (Blockwise) Window Attention 从头预训练
- [[@2023ExtendingContextWindowChen|Extending Context Window of Large Language Models via Positional Interpolation, 2023]]
    - 通过位置 id 的内插（量化）+微调实现更长的上下文长度

**有效利用长文本（Improving LLMs’ Utilization of Long Text）：**
- 暂无

**benchmark：**
- [[@2020LongRangeArenaTay|LRA]]：较早期的工作，被认为不太合理
- [[@2023NeuralNetworksChomskyDeletang|Neural Networks and the Chomsky Hierarchy, 2023]]
- [[@2023LongBenchBilingualMultitaskBai|LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding, 2023]]
    - [THUDM/LongBench: LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding (github.com)](https://github.com/THUDM/LongBench)

- [[@2023LearningCompressPromptsMu|Learning to Compress Prompts with Gist Tokens, 2023]]
- [[@2023UnlimiformerLongRangeTransformersBertsch|Unlimiformer: Long-Range Transformers with Unlimited Length Input, 2023]]
- [[@2023DissectingTransformerLengthChi|Sandwich]]（[[@2023DissectingTransformerLengthChi|Dissecting Transformer Length Extrapolation via the Lens of Receptive Field Analysis, 2023]]）
- [[@2022ExploringLengthGeneralizationAnil|Exploring Length Generalization in Large Language Models, 2022]]
- [[@2023LandmarkAttentionRandomAccessMohtashami|Landmark Attention: Random-Access Infinite Context Length for Transformers, 2023]]
- [[@2023FocusedTransformerContrastiveTworkowski|LongLLaMA]]（[[@2023FocusedTransformerContrastiveTworkowski|Focused Transformer: Contrastive Training for Context Scaling, 2023]]）
- [[@2023PoSEEfficientContextZhu|PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training, 2023]]
    - 扩展 PE（2048 -> 4096），和 [[@2023ExtendingContextWindowChen|interpolation]] 一样需要 fine tune 整个模型。但 fine tune 的时候不需要把窗口扩展（保持 2048 不变），把中间去掉一些 position（skipped position）。
- [[@2023EfficientStreamingLanguageXiao|Efficient Streaming Language Models with Attention Sinks, 2023]]

PE：
- [[@2021RethinkingPositionalEncodingKe|Rethinking Positional Encoding in Language Pre-training, 2021]]
- [[@2023ImpactPositionalEncodingKazemnejad|The Impact of Positional Encoding on Length Generalization in Transformers, 2023]]

工程：
- [[@2023BlockwiseParallelTransformerLiu|Blockwise Parallel Transformer for Long Context Large Models, 2023]]
    - 这篇也是针对 memory 的系统工程方案。大概思想应该和 flash attention 一样，把 input 分块，这样 attention 内存可以剩。这里应该是说 feed forward 也可以从分块过程中剩内存(但看他们代码好像也没有些 CUDA，直接用 jax 实现。。)

 [压缩你的 Prompt，让 LLMs 处理多达 2 倍的 Context - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/625440016?utm_medium=social&utm_oi=30536802238464&utm_psn=1635194469631832064&utm_source=wechat_timeline&utm_id=0)

结合 in-context：
- [[@2022StructuredPromptingScalingHao|Structured Prompting: Scaling In-Context Learning to 1,000 Examples, 2022]]

综述：
- [[@2022EfficientTransformersSurveyTay|Efficient Transformers: A Survey, 2022]]

 [Transformer 升级之路：7、长度外推性与局部注意力 - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/9431)

如何实现更长的输出：

| 方法                       | 代表工作                       |                            计算量                            | 使用难度 | 优点                         | 缺点                                                                    | 潜在长度上限                               |
|:-------------------------- |:------------------------------ |:------------------------------------------------------------:|:-------- |:---------------------------- |:----------------------------------------------------------------------- |:------------------------------------------ |
| 基于状态迭代的方法         | TransformerXL,RMT              |                           $O(n^2)$                           | 简单     | 计算量随着输入长度增加不明显 | 没有经过大模型预训练验证                                                | 1 million(?)                               |
| 基于位置编码外推能力的方法 | ALiBi<br>xPos<br>Unlimiformer  |                       $O(\frac NLL^2)$                       | 简单     | 训练时代价小                 | -                                                                       | MPT 模型验证了 ALiBi 在 64K 训练，可以外推到 84K |
| 基于工程优化的方法         | FlashAttention                 |                           $O(n^2)$                           | 简单     | 完全无损的 attention 替换      | 只能在特定维度上使用，不过这些特定维度目前也满足需求                    | 128k                                       |
| 基于高效 Attention 算法      | Reformer<br>Linformer<br>Flash | $\begin{array}{c}O(n) \\\downarrow \\O(n \log n)\end{array}$ | 简单     | 计算量小、省显存             | 比较缺乏高效算子、大部分都针对 Encoder 进行优化、没有经过大模型预训练验证 | -                                          |
| 其他方法                   | S4<br>FLASH                    |                              -                               | -        | 计算量小、省显存             | 缺乏高效算子、缺乏大规模模型训练验证                                    | -                                          |

> from ADL138 5.26 基础模型的创新网络架构 颜航

eval：
- language modeling (PPL)：
    - 和 long ctx 没有直接关联
        - 相距越远的 token，相关性越弱（ [Transformer 升级之路：1、Sinusoidal 位置编码追根溯源 - 科学空间|Scientific Spaces](https://spaces.ac.cn/archives/8231#%E8%BF%9C%E7%A8%8B%E8%A1%B0%E5%87%8F) ）
    - 作为基础测试，测试模型是否崩溃。
    - [[@2022TrainShortTestPress|ALiBi]]
- synthetic retrieval tasks：基于 LLM 对话能力
    - Passkey Retrieval（[[@2023LandmarkAttentionRandomAccessMohtashami|Landmark Attention: Random-Access Infinite Context Length for Transformers]]，[[@2023ExtendingContextWindowChen|Extending Context Window of Large Language Models via Positional Interpolation]]）
    - LongEval（ [How Long Can Open-Source LLMs Truly Promise on Context Length? | LMSYS Org](https://lmsys.org/blog/2023-06-29-longchat/#evaluation-toolkits-longeval) ）
- synthetic reasoning tasks：
    - Long ListOps（[[@2020LongRangeArenaTay|LRA]]）
    - [[@2023NeuralNetworksChomskyDeletang|Neural Networks and the Chomsky Hierarchy]]：需要从零开始训练
- real-world long-context tasks
    - Summarization
        - GovReport（[[@2023ExtendingContextWindowChen|Extending Context Window of Large Language Models via Positional Interpolation]]）
    - QA
        - TriviaQA（[[@2022SimpleLocalAttentionsXiong|Simple Local Attentions Remain Competitive for Long-Context Tasks]]）
        - QasperQA（ [How Long Can Open-Source LLMs Truly Promise on Context Length? | LMSYS Org](https://lmsys.org/blog/2023-06-29-longchat/#long-sequence-question-answer-benchmark) ）
    - classification
        - Hyperpartisan（[[@2022SimpleLocalAttentionsXiong|Simple Local Attentions Remain Competitive for Long-Context Tasks]]）
    - MT-bench（ [How Long Can Open-Source LLMs Truly Promise on Context Length? | LMSYS Org](https://lmsys.org/blog/2023-06-29-longchat/#human-preference-benchmark-mt-bench) ）
- Few-shot in-context learning（[[@2023FocusedTransformerContrastiveTworkowski|Focused Transformer: Contrastive Training for Context Scaling]]
