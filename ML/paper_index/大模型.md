综述：
- [[@2021SurveyTransformersLin|A Survey of Transformers, 2021]]
- Transformer models: an introduction and catalog
- [[@2024SurveyLargeLanguageZhao|A Survey of Large Language Models, 2024]]

- [[@2017AttentionAllYouVaswani|Transformer]]
- [[@2019BERTPretrainingDeepDevlin|BERT]]
- GPT
    - [[@2018ImprovingLanguageUnderstandingRadford|GPT1]]
    - [[@2019LanguageModelsAreRadford|GPT2]]
    - [[@2020LanguageModelsAreBrown|GPT3]]
- [[@2022TrainingComputeOptimalLargeHoffmann|Chinchilla]]
- [[@2023LLaMAOpenEfficientTouvron|LLaMA]]

In-context
Positional Embedding：
- [[@2022RoFormerEnhancedTransformerSu|RoPE]]

大模型训练：
- 涌现性：
    - [[@2022EmergentAbilitiesLargeWei|Emergent Abilities of Large Language Models, 2022]]
    - [[@2023AreEmergentAbilitiesSchaeffer|Are Emergent Abilities of Large Language Models a Mirage?, 2023]]
- [[@2022TrainingComputeOptimalLargeHoffmann|Chinchilla]]（[[@2022TrainingComputeOptimalLargeHoffmann|Training Compute-Optimal Large Language Models, 2022]]）
- 框架：
    - [[@2020ZeROMemoryOptimizationsRajbhandari|ZeRO]]
    - 序列并行：[[@2022ReducingActivationRecomputationKorthikanti|Reducing Activation Recomputation in Large Transformer Models, 2022]]
- [[@2016TrainingDeepNetsChen|Training Deep Nets with Sublinear Memory Cost, 2016]]：时间换空间，$O(\sqrt N)$ 的内存开销

Mamba：
- [Mamba: The Easy Way (jackcook.com)](https://jackcook.com/2024/02/23/mamba.html)
- [The Annotated S4 (srush.github.io)](https://srush.github.io/annotated-s4/)
- [Mamba: The Hard Way (srush.github.io)](https://srush.github.io/annotated-mamba/hard.html)
