Famous long context models

- [[@2023ExtendingContextWindowChen|RoPE-PI]] + direct fine-tuning:
    - [[@2023ExtendingContextWindowChen|RoPE-PI]] (up to 32K)
        - base model: Llama1. Data: the Pile. Task: language modeling
        - Eval data: PG-19 / proof-pile. Sliding Window eval stride 256
        - settings: A100x128, 1000 steps, 4B token for 32K
    - [LongChat-7B/13B-16K](https://lmsys.org/blog/2023-06-29-longchat/#step-2-finetuning-on-curated-conversation-data)
        - base model: Llama1. Data: Vicuna + [FastChat](https://github.com/lm-sys/FastChat) pipeline. Task: language modeling
        - FT&test 16k
        - cost: A100 \$3/h 7B: \$300 13B: \$700
    - [[@2024LongLoRAEfficientFinetuningChen|LongLoRA]]
        - base model: Llama2
        - settings: A100x8, 1000 steps, 2B token for 32K
    - NTK-aware
    - [[@2023YaRNEfficientContextPeng|YaRN]]
- [[@2023EffectiveLongContextScalingXiong|ABF]]
    - [[@2023EffectiveLongContextScalingXiong|ABF]]
        - 
    - CodeLLaMA
        - base model: Llama2, $b'=100b=10^6$
        - FT 16k, test 100k+, 10k steps, 20B tokens for 13B model
    - [[@2024ExtendingLLMsContextZhanga|Extending LLMs' Context Window with 100 Samples, 2024]]
    - [[@2024DataEngineeringScalingFu|Data Engineering for Scaling Language Models to 128K Context, 2024]]
