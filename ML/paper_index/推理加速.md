blog：
- [Towards 100x Speedup: Full Stack Transformer Inference Optimization](https://yaofu.notion.site/Towards-100x-Speedup-Full-Stack-Transformer-Inference-Optimization-43124c3688e14cffaf2f1d6cbdf26c6c) by Yao Fu
- [SGLang Team：在 96 个 H100 GPU 上部署具有 PD 分解和大规模专家并行性的 DeepSeek](https://mp.weixin.qq.com/s/DJpuqJnTCelMvNerDD2_Og)

- [[@2024FasterCascadesSpeculativeNarasimhan|Faster Cascades via Speculative Decoding, 2024]]
    - 结合cascade和投机解码

- MOE
    - [[@2024InferenceoptimalMixtureofExpertLargeYun|Toward Inference-optimal Mixture-of-Expert Large Language Models, 2024]]
        - MoE scaling law
    - [[@2024EfficientInferenceMixtureHuang|Toward Efficient Inference for Mixture of Experts, 2024]]
    
- 量化：
    - DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs
        - 涉及 FFN 激活分布的分析

- 稀疏性
    - [[@2025SparseVideoGenAcceleratingXi|Sparse VideoGen: Accelerating Video Diffusion Transformers with Spatial-Temporal Sparsity, 2025]]
        - 指标：注意力得分，方法：应用recency模版，保留注意力差异小的。
    