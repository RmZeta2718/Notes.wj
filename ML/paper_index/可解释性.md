---
~
---
- blog
    - [解读 GPT：logit 透镜 — LessWrong](https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens)
    - [Transformer 权重矩阵的奇异值分解具有高度可解释性 — LessWrong](https://www.lesswrong.com/posts/mkbGjzxD8d8XqKHzA/the-singular-value-decompositions-of-transformer-weight)
    - [深度学习模型可能是秘密的（几乎）线性的 — LessWrong](https://www.lesswrong.com/posts/JK9nxcBhQfzEgjjqe/deep-learning-models-might-be-secretly-almost-linear)
    - [JShollaj/awesome-llm-interpretability: A curated list of Large Language Model (LLM) Interpretability resources.](https://github.com/JShollaj/awesome-llm-interpretability)

probing

- [[@2021ConditionalProbingMeasuringHewitt|Conditional probing: Measuring usable information beyond a baseline, 2021]]

注意力模式分析：

- [[@2019WhatDoesBERTClark|What Does BERT Look At? An Analysis of BERT's Attention, 2019]]

[[@2024HowLargeLanguageZhao|How do Large Language Models Handle Multilingualism?, 2024]]

- [[@2024UnveilingLinguisticRegionsZhang|Unveiling Linguistic Regions in Large Language Models, 2024]]
    - 控制语言能力的参数
- [[@2025DissectingQueryKeyInteractionPan|Dissecting Query-Key Interaction in Vision Transformers, 2025]]
    - QK的SVD分解
- [[@2025MassiveValuesSelfAttentionJin|Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding, 2025]]
    - RoPE模型（而非其他PE）观察到巨大激活值，是retrieval任务的关键激活值

激活分析：

- [[@2024MassiveActivationsLargeSun|Massive Activations in Large Language Models, 2024]]
    - 分析模型中的巨大激活值。集中在开头和分隔符（sec 2.2）。实现了 attn 的 bias 功能。应该与 PE/lctx 存在关联
- [[@2025SparsingLawLargeLuo|Sparsing Law: Towards Large Language Models with Greater Activation Sparsity, 2025]]

语义知识：
- [[@2021TransformerFeedForwardLayersGeva|Transformer Feed-Forward Layers Are Key-Value Memories, 2021]]
- [[@2024DiscoveringLatentKnowledgeBurnsa|Discovering Latent Knowledge in Language Models Without Supervision, 2024]]

可解释性：

- 工具：[[@2023UsingCaptumExplainMiglani|Using Captum to Explain Generative Language Models, 2023]]
- [Toy Models of Superposition (transformer-circuits.pub)](https://transformer-circuits.pub/2022/toy_model/index.html)
- [Towards Monosemanticity: Decomposing Language Models With Dictionary Learning (transformer-circuits.pub)](https://transformer-circuits.pub/2023/monosemantic-features)
- Is Attention Explanation? An Introduction to the Debate
    - 扰动激活值，查看对输出分布的影响
- Backward Lens: Projecting Language Model Gradients into the Vocabulary Space
- Pre-trained Large Language Models Use Fourier Features to Compute Addition

    - 用傅里叶级数来解释 LLM
    - Composing Global Optimizers to Reasoning Tasks via Algebraic Objects in Neural Nets
    - 【田渊栋博士：传统符号推理和大模型推理的融合之路-哔哩哔哩】【视频标记点 42:41】 [https://b23.tv/Mfvi0S4](https://b23.tv/Mfvi0S4)

- Empirical Insights on Fine-Tuning Large Language Models for Question-Answering
    - 模型越 SFT 越差，甜点数据量在 60~100 条左右

Tools
- [TransformerLensOrg/TransformerLens: A library for mechanistic interpretability of GPT-style language models](https://github.com/TransformerLensOrg/TransformerLens)