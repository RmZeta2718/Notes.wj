
量化：

- DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs
    - 设计 FFN 激活分布的分析

梯度方法：

training dynamics：

- [[@2025OptimizationGeneralizationTwolayerLi|On the Optimization and Generalization of Two-layer Transformers with Sign Gradient Descent, 2025]]
- Scan and snap: Understanding training dynamics and token composition in 1-layer transformer
- The Evolution of Statistical Induction Heads: In-Context Learning Markov Chains
    - formation of induction heads on the task of learning Markov chains in context, focuses on two steps of gradient descent on a simplified linear transformer model, for Markov chains over two states.
    - transformers trained to learn n-grams in-context undergo a sequential learning procedure, by first predicting using the unigram counts, then the bigram counts, and so on. (quote [[@2025OptimizationGeneralizationTwolayerLi|On the Optimization and Generalization of Two-layer Transformers with Sign Gradient Descent]])
- [[@2024TransformersSupportVectorTarzanagh|Transformers as Support Vector Machines, 2024]]
    - 理解 attn
- [[@2023BirthTransformerMemoryBietti|Birth of a Transformer: A Memory Viewpoint, 2023]]
    - showing the development of induction heads to learn bigrams in-context and showcasing a connection with associative memories （quote [[@2024SelectiveInductionHeadsDAngelo|Selective induction Heads: How Transformers Select Causal Structures in Context]]）

On the Expressive Power of Self-Attention Matrices

- [关于维度公式“n > 8.33 log N”的可用性分析 - 科学空间|Scientific Spaces (kexue.fm)](https://kexue.fm/archives/8711)

HiPPO: Recurrent Memory with Optimal Polynomial Projections


softmax is not enough (for sharp out-of-distribution)

学习率与loss的关系：
- [[@2025MultiPowerLawLossLuo|A Multi-Power Law for Loss Curve Prediction Across Learning Rate Schedules, 2025]]

- [[@2025LanguageModelsAreMarro|Language Models Are Implicitly Continuous, 2025]]
    - token mixup 观察出了连续性变化
