probing
- [[@2021ConditionalProbingMeasuringHewitt|Conditional probing: Measuring usable information beyond a baseline, 2021]]

注意力模式分析：
- [[@2019WhatDoesBERTClark|What Does BERT Look At? An Analysis of BERT's Attention, 2019]]

[[@2024HowLargeLanguageZhao|How do Large Language Models Handle Multilingualism?, 2024]]
[[@2024UnveilingLinguisticRegionsZhang|Unveiling Linguistic Regions in Large Language Models, 2024]]

可解释性：
- 工具：[[@2023UsingCaptumExplainMiglani|Using Captum to Explain Generative Language Models, 2023]]
- [Toy Models of Superposition (transformer-circuits.pub)](https://transformer-circuits.pub/2022/toy_model/index.html)
- [Towards Monosemanticity: Decomposing Language Models With Dictionary Learning (transformer-circuits.pub)](https://transformer-circuits.pub/2023/monosemantic-features)
- Is Attention Explanation? An Introduction to the Debate
    - 扰动激活值，查看对输出分布的影响

梯度方法：


On the Expressive Power of Self-Attention Matrices
- [关于维度公式“n > 8.33 log N”的可用性分析 - 科学空间|Scientific Spaces (kexue.fm)](https://kexue.fm/archives/8711)