probing
- [[@2021ConditionalProbingMeasuringHewitt|Conditional probing: Measuring usable information beyond a baseline, 2021]]

注意力模式分析：
- [[@2019WhatDoesBERTClark|What Does BERT Look At? An Analysis of BERT's Attention, 2019]]

[[@2024HowLargeLanguageZhao|How do Large Language Models Handle Multilingualism?, 2024]]
[[@2024UnveilingLinguisticRegionsZhang|Unveiling Linguistic Regions in Large Language Models, 2024]]

激活分析：
- Sparsing Law: Towards Large Language Models with Greater Activation Sparsity


可解释性：
- 工具：[[@2023UsingCaptumExplainMiglani|Using Captum to Explain Generative Language Models, 2023]]
- [Toy Models of Superposition (transformer-circuits.pub)](https://transformer-circuits.pub/2022/toy_model/index.html)
- [Towards Monosemanticity: Decomposing Language Models With Dictionary Learning (transformer-circuits.pub)](https://transformer-circuits.pub/2023/monosemantic-features)
- Is Attention Explanation? An Introduction to the Debate
    - 扰动激活值，查看对输出分布的影响
- Backward Lens: Projecting Language Model Gradients into the Vocabulary Space
- Pre-trained Large Language Models Use Fourier Features to Compute Addition
    - 用傅里叶级数来解释LLM
    - Composing Global Optimizers to Reasoning Tasks via Algebraic Objects in Neural Nets
    - 【田渊栋博士：传统符号推理和大模型推理的融合之路-哔哩哔哩】【视频标记点 42:41】 [https://b23.tv/Mfvi0S4](https://b23.tv/Mfvi0S4)

- Empirical Insights on Fine-Tuning Large Language Models for Question-Answering
    - 模型越SFT越差，甜点数据量在60~100条左右


量化：
- DuQuant: Distributing Outliers via Dual Transformation Makes Stronger Quantized LLMs
    - 设计FFN激活分布的分析

梯度方法：


On the Expressive Power of Self-Attention Matrices
- [关于维度公式“n > 8.33 log N”的可用性分析 - 科学空间|Scientific Spaces (kexue.fm)](https://kexue.fm/archives/8711)