- [ ] è§†é¢‘ç¬”è®° [ä¿¡æ¯é‡ ï½œç†µ ï½œ äº¤å‰ç†µ ï½œKL æ•£åº¦ ï¼ˆç›¸å¯¹ç†µï¼‰ï½œäº¤å‰ç†µæŸå¤±å‡½æ•°_å“”å“©å“”å“©_bilibili](https://www.bilibili.com/video/BV1L8411X7ZZ/?spm_id_from=333.999.top_right_bar_window_history.content.click&vd_source=0351845a83270d548d966eeb2ab72c06)
- [ ] ä»£ç ç¬”è®° resize token embedding
- [ ] äº†è§£ DummyScheduler æ˜¯æ€ä¹ˆå½±å“ optimizer parameter group çš„
- [ ] äº†è§£ flash attention
- [ ] GPT2+flash attention
- [ ] trainer scheduler min value https://github.com/huggingface/transformers/issues/26209
- [ ] å¤§æ•°æ®é›† `load_dataset` ä¸ºä»€ä¹ˆèŠ±è¿™ä¹ˆé•¿æ—¶é—´ï¼ˆ[Slow dataloading with big datasets issue persists Â· Issue #2252 Â· huggingface/datasets (github.com)](https://github.com/huggingface/datasets/issues/2252)ï¼‰
- [ ] [Long-Contextä¸‹LLMæ¨¡å‹æ¶æ„å…¨é¢ä»‹ç» (qq.com)](https://mp.weixin.qq.com/s/HjIRk0BzhRAB_4yfpiI2jQ)

https://self-supervised.cs.jhu.edu/

Retrieval-based Language Modelsï¼š
- https://ai.stanford.edu/blog/retrieval-based-NLP/
- https://jalammar.github.io/illustrated-retrieval-transformer/

Retrieval from Memory

[[@2021ZeroshotLabelAwareEventZhang|Zero-shot Label-Aware Event Trigger and Argument Classification, 2021]]

[[@2018ZeroShotTransferLearningHuang|Zero-Shot Transfer Learning for Event Extraction, 2018]]

SimCSE: Simple Contrastive Learning of Sentence Embeddings

Semi-supervised New Event Type Induction and Event Detection

CLEVE: Contrastive Pre-training for Event Extraction

CS224n
- [A4](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/assignments/a4.pdf)
- [A5](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/assignments/a5.pdf)
- [final1](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/project/default-final-project-handout-squad-track.pdf)  [final2](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1214/project/default-final-project-handout-robustqa-track.pdf)

ç»¼è¿°
- [[@2021SurveyTransformersLin|A Survey of Transformers, 2021]]
- [[@2023CookbookSelfSupervisedLearningBalestriero|A Cookbook of Self-Supervised Learning, 2023]]

[[@2022RoFormerEnhancedTransformerSu|RoPE]]

LangChain [Welcome to LangChain â€” ğŸ¦œğŸ”— LangChain 0.0.142](https://python.langchain.com/en/latest/index.html)
