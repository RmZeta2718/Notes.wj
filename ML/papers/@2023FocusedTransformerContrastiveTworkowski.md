---
aliases: ["Focused Transformer: Contrastive Training for Context Scaling", "Focused Transformer: Contrastive Training for Context Scaling, 2023", "LongLLaMA"]
---
# Focused Transformer: Contrastive Training for Context Scaling

- **Journal**: arxiv:2307.03170
- **Author**: Szymon Tworkowski, Konrad Staniszewski, Mikołaj Pacek, Yuhuai Wu, Henryk Michalewski, Piotr Miłoś
- **Year**: 2023
- **URL**: http://arxiv.org/abs/2307.03170
- [**Zotero**](zotero://select/items/@2023FocusedTransformerContrastiveTworkowski)
- [**ReadPaper**](https://readpaper.com/pdf-annotate/note?pdfId=4776164222732075009&noteId=1868450996475092736)

## 论文试图解决的问题

%% 是否是新的问题。现状、难点。motivation %%

## 论文的总体贡献

- 提出分心问题（distraction issue），
- 通过Focused Transformer（FoT）来缓解分心问题
- 可以在不修改模型的情况下增强现有模型的记忆能力，即在现有模型上Finetune

## 论文提供的关键元素、关键设计

### 总体流程

## 实验

%% 实现细节、设置。数据集。评估。消融实验 %%

## 后续工作

%% 有什么疑问。如何继续深入。如何吸取到你的工作中 %%

## 相关研究

%% 如何归类。值得关注的研究员 %%
