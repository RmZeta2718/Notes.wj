---
aliases:
  - "Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding"
  - "Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding, 2025"
---
# Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding

- **Journal**: arXiv:2502.01563 #ICML/25
- **Author**: Mingyu Jin, Kai Mei, Wujiang Xu, Mingjie Sun, Ruixiang Tang, Mengnan Du, Zirui Liu, Yongfeng Zhang
- **Year**: 2025
- **URL**: http://arxiv.org/abs/2502.01563
- [**Zotero**](zotero://select/items/@2025MassiveValuesSelfAttentionJin)

## 论文试图解决的问题

%% 是否是新的问题。现状、难点。motivation %%

[ICML 2025 | 注意力机制中的极大值：破解大语言模型上下文理解的关键](https://mp.weixin.qq.com/s/HagJ7UWDi3vsH9LeIVXtmA)

## 论文的总体贡献

## 论文提供的关键元素、关键设计

### 总体流程

## 实验

%% 实现细节、设置。数据集。评估。消融实验 %%

## 相关研究

%% 如何归类。值得关注的研究员 %%
