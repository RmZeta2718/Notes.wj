---
aliases:
  - Effective Long-Context Scaling of Foundation Models
  - Effective Long-Context Scaling of Foundation Models, 2023
  - ABF
---
# Effective Long-Context Scaling of Foundation Models

- **Journal**: arxiv:2309.16039 #NAACL/24
- **Author**: Wenhan Xiong, Jingyu Liu, Igor Molybog, Hejia Zhang, Prajjwal Bhargava, Rui Hou, Louis Martin, Rashi Rungta, Karthik Abinav Sankararaman, Barlas Oguz, Madian Khabsa, Han Fang, Yashar Mehdad, Sharan Narang, Kshitiz Malik, Angela Fan, Shruti Bhosale, Sergey Edunov, Mike Lewis, Sinong Wang, Hao Ma
- **Year**: 2023
- **URL**: http://arxiv.org/abs/2309.16039
- [**Zotero**](zotero://select/items/@2023EffectiveLongContextScalingXiong)
- [**ReadPaper**](https://readpaper.com/pdf-annotate/note?pdfId=4822670164549959681&noteId=2281115421933392384) 

## 论文试图解决的问题

%% 是否是新的问题。现状、难点。motivation %%

## 论文的总体贡献

## 论文提供的关键元素、关键设计

### 总体流程

![fig8](https://pdf.cdn.readpaper.com/parsed/fetch_target/3079341e2b8d28d16e121f438d9f884b_18_Figure_8_-1432786904.png)

## 实验

%% 实现细节、设置。数据集。评估。消融实验 %%

### Training Curriculum

Does pretraining from scratch with long sequences yield better performance than continual pretraining?

总token保持一致（400B）、每个batch token保持一致

![tab10](https://pdf.cdn.readpaper.com/parsed/fetch_target/88dab9ac2f6dad3f27f5b77aacbf3d1f_9_Table_10_-1692317328.png)

![fig6](https://pdf.cdn.readpaper.com/parsed/fetch_target/88dab9ac2f6dad3f27f5b77aacbf3d1f_10_Figure_6_-830575847.png)

## 相关研究

%% 如何归类。值得关注的研究员 %%
