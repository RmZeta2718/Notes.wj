---
aliases: ["BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, 2019", "BERT"]
---
# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

- **Journal**: arXiv:1810.04805 [cs]
- **Author**: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova
- **Year**: 2019
- **URL**: http://arxiv.org/abs/1810.04805
- [**Zotero**](zotero://select/items/@2019BERTPretrainingDeepDevlin)
- [**ReadPaper**](https://readpaper.com/pdf-annotate/note?noteId=744434043595288576)
- [bilibili](https://www.bilibili.com/video/BV1PL411M7eQ/)


## 论文试图解决的问题（是否是新的问题）



## 论文的总体贡献



## 论文提供的关键元素、关键设计

### 总体流程



## 实验（设置、数据集）

### 实现细节



### 数据集



### 评估



### 消融实验



## 有什么疑问，如何继续深入，如何吸取到你的工作中



## 相关研究（如何归类，值得关注的研究员）

- ELMo：双向信息，但是是RNN，无监督，基于特征
- 
- GPT：单向信息，Transformer

