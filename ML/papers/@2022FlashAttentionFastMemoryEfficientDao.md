---
aliases:
  - "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness"
  - "FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness, 2022"
  - FlashAttention
---
# FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness

- **Journal**: arxiv:2205.14135 #NeurIPS/22  
- **Author**: Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher Ré
- **Year**: 2022
- **URL**: http://arxiv.org/abs/2205.14135
- [**Zotero**](zotero://select/items/@2022FlashAttentionFastMemoryEfficientDao)
- [**ReadPaper**](https://readpaper.com/pdf-annotate/note?pdfId=4667276321932460033&noteId=1857762696672213760)

## 论文试图解决的问题

%% 是否是新的问题。现状、难点。motivation %%

## 论文的总体贡献

## 论文提供的关键元素、关键设计

### 总体流程

## 实验

%% 实现细节、设置。数据集。评估。消融实验 %%

## 相关研究

%% 如何归类。值得关注的研究员 %%

[[@2021DataMovementAllIvanov|Data Movement Is All You Need: A Case Study on Optimizing Transformers, 2021]]

